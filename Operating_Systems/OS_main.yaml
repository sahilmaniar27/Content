---
Subject:
  name: Operating Systems
  subtopics:
    - Introduction
    - Process & Threads
    - Scheduling
    - Memory & Address Space
    - Virtual memory & paging
    - Concurrency
    - Synchronization
    - Persistence
    - Miscellaneous
Process & Threads:
  - Process:
    Concept of Process and Process Attributes:
      References:
        - Gate Lectures by Ravindrababu Ravula: https://youtu.be/ucVm_arB-fw
        - Neso Academy: https://youtu.be/OrM7nZcxXZU
        - Operating System Concepts Peter B. Galvin: Chapter 3, section 3.1
      Practice_questions:
        - Question_1: |
            State true or false.
            A process can be multiple states at a time.
          Company_tags: None
          Level: None
          Correct_answer: "False"
          Answers:
            - Hint: A process can only exist in 1 state at a time.
            - Solution: A process can only be in one state at a time. As in, it can be in a ready state or a blocked state or swapped state, etc.
        - Question_2: |
            A file is opened by the user. What will be the changes in Local Descriptor Table(LDT) and Global Descriptor Table(GDT)?
            Moreover, if two different processes have opened one instance each of a file and one of the processes close it,
            what will be the changes in the LDT and GDT?
          Company_tags: None
          Level: None
          Answer:
            Hint: GDT has an entry, even if one instance is there.
            Solution: |
                LDT is also called a per-process descriptor table, and GDT is called a system-wide descriptor table.When you initially open a
                file, both LDT and GDT make new entries of the file. However, if it is already opened by even one process, the GDT only increases
                the counter. If an instance of a file opened by another process and a new process opens it, a new instance is made in LDT.
                When a file is closed by one of the processes, given that only a single instance is opened by each process, only the
                counter in GDT is decreased. However, in LDT, the entry is deleted (as only one instance was opened, and that too was
                closed). There is an entry in the GDT for a file as long as there is at least one instance of the file opened by a process.
        - Question_3: |
            Which of the following is correct about Page Table?
            S1: It keeps track of the main memory associated with the process.
            S2: It keeps track of secondary memory to processes.
            S3: It keeps the information needed to manage Physical Memory.
            S4: It keeps track of protection attributes of main and secondary memory.
          Company_tags: None
          Level: None
          Options:
            - 1: S1
            - 2: S2
            - 3: S3
            - 4: S4
          Correct_answer: S3
          Answer:
            - Reference: https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/8_MainMemory.html (It's section 8.1.3)
            - Hint: The OS only generates Logical Address.
            - Solution: |
                S3 is wrong because OS only generates Logical Address, so Memory Table, also known as a page table, keeps track of virtual
                memory of the process. Note that user programs never see physical addresses. User programs work entirely in logical
                address space, and any memory references or manipulations are done using purely logical addresses. Only when the address
                gets sent to the physical memory chips is the physical memory address generated.

        - Question_4: Which of the following attributes of a thread are private to itself?
          Company_tags: None
          Level: None
          Options:
            -1: Heap Space
            -2: Stack Space
            -3: File Descriptors
            -4: Signal Handler
          Correct_answer: 2
          Answer:
            - Hint:
            - Solution: |
                A thread shares with its peer threads its Code section Global data section, Operating-system resources, Process instructions,
                Open files (descriptors), Signal and signal handlers, Current working directory, and User and group id. But, Stack space,
                Signal Masks and Register Set is private for a thread. It also has a separate return value for itself (errno).

        - Question_5: Which of the following is an indirect way of communication between processes (IPC)?
          Company_tags: None
          Level: None
          Options:
            -1: Shared Memory
            -2: Signals
            -3: Mailbox
            -4: Passing Pointers
          Correct_answer: 3
          Answer:
            - Hint:
            - Solution: |
                The correct option is c. We pass signals explicitly to the processes we need to communicate to, and shared memory is with the
                passing pointers. Mailbox is called a port. It consists of a queue of messages. The sender keeps the message in the processes
                we want to share it with (processes need to join themselves to the shared memory block). Similar is the passing pointers.
                Mailbox is called a port. It consists of a queue of messages. The sender keeps the message in the
                mailbox, and the receiver picks them up.
    Fork and Exec system calls:
      References:
        - Neso Academy (YouTube): https://www.youtube.com/watch?v=IFEFVXvjiHY
        - Techtud (YouTube): https://www.youtube.com/watch?v=PwxTbksJ2fo
        - Operating System Concepts by Abraham Silberschatz, Peter B. Galvin, and Greg Gagne: Chapter 3, Section 3.3
        - GeeksForGeeks: https://www.geeksforgeeks.org/difference-fork-exec/
      Practice_questions:
        - Question_1: How is PID assigned to a process?
          Company_tags: None
          Level: None
          Options:
            - 1: Randomly
            - 2: It is the next vacant PID
            - 3: It is the next vacant PID from the start
            - 4: It is the least vacant PID
          Correct_answer: 2
          Answer:
            - Hint: Go through how PID is assigned
            - Solution: |
                PID is always assigned as the next vacant PID. For ex. if the current PID is x, the next PID will be the next vacant PID slot after x.
                It may be x+1, x+2 or even 3, 4, etc. It is determined by a circular search.
        - Question_2: |
            Read the following piece of code carefully.
            int main(){
              printf("Hello, I am in main process");
              char* args = "Hello World\0";
              execv("./prog1",args);
              printf("Execv exected. Exiting main process");
            }
            Will the second print statement be executed?
          Company_tags: None
          Level: None
          Reference:
            - https://linuxhint.com/linux-exec-system-call/
          Answer:
            - Hint: Exec repalces the process that calls it.
            - Solution: |
                No. the second print statement will not be printed. It is because the exec() system call replaces the process that calls it.
                So, when exec() will be executed, it will execute the code that it's .c file contains. The main process gets replaced so it
                does not exist from the moment the exec() system call starts executing.
                Also remember, as the process created by exec() is assigned the PID of the calling process, it also replaces the address space
                of the calling process. It is the reason why we say that the original process does not exist after the exec system call.
        - Question_3: |
            If a process has PID as 'x', suppose it executes exec() at some pint of time.
            What will be the PID of the process (created due to the exec() call)?
          Company_tags: None
          Level: None
          Options:
            - 1: x
            - 2: anything greater than x
            - 3: anything less than x
            - 4: Can't say
          Correct_answer: 1
          Answer:
            - Hint: exec() replaces the calling process.
            - Solution: |
                The PID of the new process created due to exec() will be 'x' itself. The explanation is simply that the exec() process replaces
                the original calling process. So, if you execute a print statement that will print the PID of the process, both will print 'x'.
        - Question_4: |
            If process P1 creates two processes P2 and P3. Who will be the previous sibling of P2 and
            who will be the next sibling of P1, respectively?
          Company_tags: None
          Level: None
          Options:
            - 1: P1 and P2 respectively
            - 2: P1 and P2 respectively
            - 3: P3 and P3 respectively
            - 4: P2 and P1 respectively
          Correct_answer: 2
          Answer:
            - Hint:
            - Solution: The next sibling of the parent is the last child it creates, and the previous sibling of the 1st child is the parent itself.
        - Question_5: What is the return value of fork()?
          Company_tags: None
          Level: None
          Answer:
            - Hint: There are two.
            - Solution: >
                It returns the PID of the child to the parent process (i.e., if the fork() is successful), else it returns '-1'.
                As for the second return value, it returns '0' to the child that is created.
        - Question_6: |
            Let there be two concurrent processes, P1 and P2.
              P1 -> {
                //some code
                x++;
                //remaining code
              }
              P2 -> {
                //some code
                x--;
                //remaining code
              }
            Let the initial value of x be zero (0). What will be minimum value of “x” after these processes complete their execution?
            (Assume the “some code” and “remaining code” do not modify x)
          Company_tags: None
          Level: None
          Option:
            -1: 0
            -2: 1
            -3: 2
            -4: -1
          Correct_answer: 4
          Answer:
            - Hint:
            - Solution: |
                The possibility of context switch at any time during the execution of x++ or x-- is the reason that makes this possible.
                “x--” essentially means,
                load 'x' in a reg; ---(1)
                Reduce the 'reg' by '1'; ---(2)
                store 'x' in the 'reg';  ---(3)
                Similar is the case with “x++". Now if there is a context switch after (1), then if “x++” executes and then (2) and (3)
                executes, the value in “reg” will be zero before context switch, and after executing “x++” also, “reg” will contain zero.
                And hence after stages 2 and 3, the final value of x will be “-1”. Similarly, the max value of x can be 1.
        - Question_7: |
            Find the total number of new processes created. Draw a process tree.
            Assume that the necessary libraries are included and that the fork() call produces child processes and does not give '-1'
            (which is return when fork does not happen successfully).
            int main()
            {
              printf("fork program starting with pid %d\n",getpid());
              if(fork())
                if(!fork())
                  fork();
              printf("My PID=%d My PPID=%d\n",getpid(),getppid());
              wait(NULL);
              wait(NULL);
              return 0;
            }
          Company_tags: None
          Level: None
          Reference: None
            - https://www.geeksforgeeks.org/creating-multiple-process-using-fork/
            - https://linuxtrainers.wordpress.com/2014/12/31/how-fork-system-call-works-what-is-shared-between-parent-and-child-process/
            - https://unix.stackexchange.com/questions/87551/which-file-in-kernel-specifies-fork-vfork-to-use-sys-clone-system-call
          Correct_answer: 3
          Answer:
            - Hint:
            - Solution: |
                Please go through the reference material for this question first before reading the solution.
                Now, for the solution part, remember that the return value for the fork is 'zero' for the child and 'the PID of the child' to the
                parent.
                When the parent process executes the fork() call, it will create a new process. The PID of the process will be the
                return value in the 'if condition' of the parent process, so it will go to the 'inner if condition'. However, as for the 1st child
                process, the value will be 'zero' in its 'if condition', this is because the return value of fork is 'zero for the child.
                Then, the child begins the execution at the same point where the fork() call returns execution to the main program. However,
                because it does not satisfy the 1st 'if condition', it goes on to print and exit. Then, the parent forks one more process.
                The child that is created will move ahead in the 'if condition' tree as '!0' evaluates 'true' for the inner 'if condition'.
                So, it goes inside that and executes one more fork(). As for the parent, it does not go ahead in the 'if condition' as '!PID'
                evaluates as false. Note that PID zero is permanently assigned to the 'swapper process' or 'sched process'. So, '!PID' will
                always always be 'false'. So, it will wait for the two children to finish execution (two because of two wait() calls). In
                this way, there will be three new processes created.
                As for the process tree, it will be like this:
                                        P1 -----> Parent Process
                                      /  \
                                     /    \
                      1st child<----P2     P3---->2nd child
                                            \
                                             P4----->1st child of 2nd child
        - Question_8: |
            What will be the value of 'x' in each print statement? Assume all processes will be created.
            int x=0;
            int main()
            {
              if(fork())
                x=x+5;
              else if(!fork())
              {
                x=x+10;
                printf("PID = %d X=%d\n",getpid(),x);
                exit(0);
              }
              else
                x=x-2;
              x=x+15;
              printf("PID=%d and X =%d\n",getpid(),x);
              while(wait(NULL)!=-1);
                return 0;
            }
          Company_tags: None
          Level: None
          Reference:
            - https://www.geeksforgeeks.org/fork-memory-shared-bw-processes-created-using/
          Answer:
            - Hint:
            - Solution: |
                The parent process forks a child process and will go inside the if condition. It will first increase 'x' by '5' and then by '15' and
                then wait till all of its children to die (because of the while loop). The 1st child will not go inside the 'if condition'
                and will go to the 'else if' statement. But, it will not go inside the else if statement because of '!fork()' condition in
                the 'else if' statement. At the same time, due to a successful fork, the 1st child will create a child of its own, and
                that child will enter the 'else if' statement. Concurrently, the 1st child of the parent will keep on going forward. So,
                it will execute the else condition and decrease the value of 'x' by '2' and then increase it by '15'. This child did not have
                the value of 'x' to be '5' initially because when it was created, the value of 'x' was 'zero'.The reason
                being, the child, contains a copy of all variables the parent had at the time of forking (except for shared
                variables, which are the same for both parent and child). Then, the 1st child of the parent process will also wait for its
                child to exit. The child of the 1st child will execute the statements inside of the 'else if' statement and exit with status
                '0'. The value of 'x' in this process (child of the child of the parent process) will thus be '0+10',i.e., '10'.
                So, the value of 'x' in parent process will be '0+5+15'='20' and that in 1st child will be '0-2+15'='13'. Here is the process
                tree:
                                                        P1-----> Parent process, x=20
                                                       /
                                                      P2------>1st child, x=13
                                                     /
                                                    P3------>child of the child of parent, x=10
        - Question_9: Why can't a forked process have a PID as 0 or 1?
          Company_tags: None
          Level: None
          Reference:
            - https://unix.stackexchange.com/questions/83322/which-process-has-pid-0
            - https://hackernoon.com/the-curious-case-of-pid-namespaces-1ce86b6bc900
          Answer:
            - Hint:
            - Solution: |
                A process can have a PID between 0 and 32767. The upper range can be found in a system's '/proc/sys/kernel/pid_max' file.
                It is generally '32767'. But, the process that is created using fork cannot be assigned a PID '0' or '1'. It is because
                the PID '0' is given to the first process that the OS creates, and this process is generally the 'swapper process' or 'sched
                process'. This process is responsible for the paging in the system. This process dies or exits when the OS shuts down. So,
                this PID is never vacant. Then, PID '1' is the 'init process'. When this process dies, all the processes are killed with the
                kill() signal. So, this is why a forked process cannot have a PID '0' or '1'.
        - Question_10: Explain in brief what happens during the fork() system call.
          Company_tags: None
          Level: None
          Reference:
          Answer:
            - Hint:
            - Solution: |
                The following steps take place when a fork() executes:
                - A slot is allocated in the process table for the new process.
                - A unique process id is assigned to the new process.
                - A copy of the process image of the parent is made, except for shared memory.
                - fork() also increases counters for any files owned by the parent, to reflect that an additional process now also owns
                  these files.
                - The kernel (during the fork() call) also assigns the child process to a ready state.
                - A Process ID number (PID) of the child to the parent process and a 0 value to the child process is returned.
                - After completing these functions, OS will do the following operations as a part of the dispatcher routine:
                  • Control returns to the user mode at the point of the fork call of the parent
                  • Transfer control to the child process. The child process begins executing at the same point in the code as the parent,
                  namely at the return from the fork call, i.e., the instruction next in line after fork().
                  • Transfer control to another process. Both child and parent are left in the ready state.
        - Question_11: What is the difference between fork() and vfork()?
          Company_tags: None
          Level: None
          Reference:
            - https://www.quora.com/What-is-the-difference-between-fork-and-vfork
          Answer:
            - Hint:
            - Solution: |
                • vfork() is a variant of fork(), and it behaves exactly like fork() except that it does not make a copy of the address
                  space and page table of the parent process.
                • Memory, including stack space, is shared by the parent and the child process.
                • vfork() suspends the parent process until the child releases its memory address space, i.e., until the child makes a call
                  to exec() or _exit().
        - Question_12: Explain Copy-On-Write (CoW) mechanism.
          Company_tags: None
          Level: None
          Reference:
            - https://unix.stackexchange.com/questions/58145/how-does-copy-on-write-in-fork-handle-multiple-fork
            - https://www.reddit.com/r/compsci/comments/31szui/trying_to_understand_fork_and_copyonwrite_cow/
          Answer:
            - Hint: Lazy approach.
            - Solution: |
                Today's systems run with hundreds and thousands of processes. If the OS goes according to the rules and copies the memory
                in use by the parent process after every fork(), it will be very time-consuming because of the context switching involved. So,
                to avoid this, the OS waits until any one of the parent or child modifies the memory. When any one of them does, it then makes
                the two copies of memory it was supposed to do at the beginning of the fork().
                Additionally, as processes reside in virtual memory, whenever the child wants to read something, it reads from the parent
                provided; it has not written since it was created. But, as soon as it writes for the first time, the
                required copy of the parent memory is made (as it should have been at the start of the fork() call), and the child's memory
                space is updated. Then the child process writes to the newly created memory space. This is CoW.
        - Question_13: Explain the difference between wait() and waipid(). What are the different parameters of the two functions?
          Company_tags: None
          Level: None
          Reference:
            - http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch08lev1sec6.html
            - https://webdocs.cs.ualberta.ca/~tony/C379/C379Labs/Lab3/wait.html
          Answer:
            - Hint:
            - Solution: |
                wait() waits for a child to terminate. It will be temporarily blocked till then. Whereas, waitpid can be either blocking or
                non-blocking:
                - If the "options" parameter is 0, then it is blocking.
                - If the "options" parameter is WNOHANG, then is it non-blocking.
                WNOHANG is one of the options of the 'options' parameter of the waitpid function.
                If more than one child is running, then wait() returns when the first time one of the child exits. Whereas, waitpid() is more
                versatile. Its 'pid' parameter provides that versatility. If:
                - pid < -1 means that it has to wait for any child process whose process group ID is equal to the absolute value of pid to
                  exit.
                - pid = -1 means that it has to wait for any child process to exit. At this point, it is equivalent to wait().
                - pid = 0 means that it has to wait for any child process whose process group ID is equal to that of the calling process.
                - pid > 0 means that it has to wait for the child whose process ID is equal to the value of pid.
                In this way, it is different from the wait() function.
        - Question_14: What is the difference between the 'exec' functions suffixed with 'p' and 'e'?
          Company_tags: None
          Level: None
          Reference:
          Answer:
            - Hint: Need of specifying path variable.
            - Solution: |
                The 'exec' functions, which end with the suffix 'p' will search the current PATH variable to find the executable file. Whereas,
                the 'exec' functions suffixed with 'e' have to be given the PATH variable as an input parameter, and then it will find the
                file to be executed in that PATH variable.
                Note: PATH variable is the address of the file in terms of directories. For example, a file can have its PATH variable as
                      "/home/mypc/Downloads/myfolder"
      Premium/Quiz_questions:
        - Question_1: >
            Consider a process with multiple threads.
            What would happen if one of the threads crashes due to fault?
          Company_tags:
          Level:
          Reference:
            - https://stackoverflow.com/questions/36423511/what-happen-if-thread-crashes-which-is-better-thread-or-process
          Comment:
          Options:
            - 1: Only, Thread with the fault crashes
            - 2: Entire process crashes
            - 3: Undefined behavior
            - 4: Cannot be predicted, either of the options can happen
          Correct_answer: 2
          Answers:
            - Hint: Threads share their address space.
            - Answer: |
                If one thread crashes due to a segmentation fault or any other error, all other threads and the entire process are killed.
                This is because the threads are part of the same process space, so it does not generally make any sense to keep going if a
                thread has caused a fault signal. A crash signal (like SIGSEGV, SIGBUS, SIGABRT) means that you have lost control over
                the behavior of the process, and anything could have happened to its memory. There is a negative side of the entire process being
                killed due to a fault in one of the threads. This situation could lead to data and system corruption if the other threads were
                in the middle of some important task. Due to this reason, important & independent tasks are generally performed as a separate
                process, such that the crashing of one does not bring down the entire functionality.
  - Thread:
    Resources:
      - GeeksForGeeks: https://www.geeksforgeeks.org/thread-in-operating-system/
      - Tutorials Point: https://www.tutorialspoint.com/operating_system/os_multi_threading.htm
      - Neso Academy: https://www.youtube.com/watch?v=LOfGJcVnvAk
    Practice_questions:
      - Question_1: What is the difference between User-level threads & kernel-level threads?
        Company_tags: None
        Level: None
        Reference:
          - www.cs.iit.edu/~cs561/cs450/ChilkuriDineshThreads/dinesh's files/User and Kernel Level Threads.html
        Correct_answer: None
        Answers:
          - Hint: None
          - Solution: |
              The user-level threads are created by the user. They reside in the user space and are managed by APIs. The APIs manage all
              the function calls made by these threads without invoking the kernel mode. Additionally, they are also invisible to the OS.
              Kernel-Level threads, as the name suggests, are created and managed by the kernel. The kernel knows everything about the threads.
              However, they are slower than the user level threads because the kernel manages and schedules threads as well as processes. Thus, it
              requires a full thread control block (TCB) for each thread to maintain information about a specific thread. As a result, there is
              significant overhead and an increase in kernel complexity.

      - Question_2: >
          Consider a process with multiple threads.
          What would happen if one of the threads crashes due to fault?
        Company_tags:
        Level:
        Reference:
          - https://stackoverflow.com/questions/36423511/what-happen-if-thread-crashes-which-is-better-thread-or-process
        Comment:
        Options:
          - 1: Only, Thread with the fault crashes
          - 2: Entire process crashes
          - 3: Undefined behavior
          - 4: Cannot be predicted, either of the options can happen
        Correct_answer: 2
        Answers:
          - Hint: Threads share their address space.
          - Solution: |
              If one thread crashes due to a segmentation fault or any other error, all other threads and the entire process are killed.
              This is because the threads are part of the same process space, so it does not generally make any sense to keep going if a
              thread has caused a fault signal. A crash signal (like SIGSEGV, SIGBUS, SIGABRT) means that you have lost control over
              the behavior of the process, and anything could have happened to its memory. There is a negative side of the entire process being
              killed due to a fault in one of the threads. This situation could lead to data and system corruption if the other threads were
              in the middle of some important task. Due to this reason, important & independent tasks are generally performed as a separate
              process, such that the crashing of one does not bring down the entire functionality.
      - Question_3: What are the benefits of using a thread, instead of using a process?
        Company_tags:
        Level:
        Reference:
          - https://www.tutorialspoint.com/operating_system/os_multi_threading.htm
        Answers:
          - Hint: Lightweight, easy to manage, etc.
          - Solution: |
              - Threads share memory & other resources of the parent process.
              - Threads are more economical to context switch.
              - Threads can run in parallel, and hence multiple tasks can be done parallelly.
              - Threads are also easy to create, switch between, and terminate.
      - Question_4: What is the difference between Data and Task Parallelism?
        Company_tags:
        Level:
        Reference:
          - https://en.wikipedia.org/wiki/Task_parallelism
        Answer:
          - Hint:
          - Solution: |
              Data Parallelism is distributing data to and running them through the same task (thread/process). Task Parallelism is running
              different tasks on different cores on the same data.
              Both these parallelisms can be visualized as follows:
              - Data Parallelism------------>
                                                    -------------------DATA-----------------
                                                  /                                          \
                                                  --------------------------------------------
                                                 |    D0    |    D1    |    D2    |    D4     |
                                                  --------------------------------------------
                                                    |             |          |          |
                                                  Core-1        Core-2     Core-3     Core-4

              - Task Parallelism------------>
                                                    -------------------DATA-----------------
                                                  /                                          \
                                                  --------------------------------------------
                                                 |                                            |
                                                  --------------------------------------------
                                                    /             /             \             \
                                                  Core-1        Core-2         Core-3       Core-4

      - Question_5: Why does the return value of a thread function be of the type "(void *)" only?
        Company_tags:
        Level:
        Reference:
          - https://en.wikipedia.org/wiki/Task_parallelism
        Answer:
          - Hint:
          - Solution: |
              #Will write soon
      - Question_6: |
          What does the "getpid()" function return when executed in a thread? Does it give an error? If not, what is the return value?
        Company_tags:
        Level:
        Reference:
          - https://stackoverflow.com/questions/9305992/if-threads-share-the-same-pid-how-can-they-be-identified
        Answer:
          - Hint: Not an error.
          - Solution: |
              No, It will not give an error. By calling the function "getpid()" inside a thread, you will get the PID of the thread that created
              it. If we execute that function in a newly created thread (by a process), you will get the PID of the process that created it. So,
              executing the "getpid()" function in a thread will give you the PID of the process that created it.
              Note: We can think of a thread as a process that shares its address space with its parent process.
      - Question_7: >
          Like a process has a PID, does a thread have something similar to PID that the LINUX system uses to identify it(that thread)?
        Company_tags:
        Level:
        Reference:
          - https://stackoverflow.com/questions/21091000/how-to-get-thread-id-of-a-pthread-in-linux-c-program
        Answer:
          - Hint: Using a System Call.
          - Solution: |
              Yes, there is a TID of a thread. We can get it by executing the system call "syscall(__NR_gettid)" in the thread (whose TID we
              want to know).
      - Question_8: If we create a thread and the return value is '0', does it mean that the creation has failed? Explain.
        Company_tags:
        Level:
        Reference:
          - https://stackoverflow.com/questions/21091000/how-to-get-thread-id-of-a-pthread-in-linux-c-program
        Answer:
          - Hint: Not failed.
          - Solution: |
              No, getting '0' as a return value does not mean that the thread creation has failed. When we create a thread using the
              "pthread_create()" function, the kernel stores the identifier of the newly created thread in the location pointed by the argument
              in the function. Furthermore, if the creation is successful, a '0' is returned.
      - Question_9: What is the "detachable state" of a thread?
        Company_tags:
        Level:
        Reference: None
        Answer:
          - Hint: Release of the holding resources.
          - Solution: |
              Detachable thread means that when it terminates, it will release all the resources that it is was allocated. A thread is in
              "Joinable State" by default. If we want to make the thread detachable, we need to make it explicitly using the "pthread_detach()"
              function and passing the thread as the argument.
      - Question_10: What happens if a thread executes the fork() call?
        Company_tags:
        Level:
        Reference:
          - https://stackoverflow.com/questions/39890363/what-happens-when-a-thread-forks
          - http://gauss.ececs.uc.edu/Courses/c694/lectures/ForksThreads/forks.html
        Answer:
          - Hint:
          - Solution: |
              A new process will be created, and the parent process will the process that created the thread that executed the fork call.
      - Question_11: Is fork call better than clone? Justify.
        Company_tags:
        Level:
        Reference:
          - https://unix.stackexchange.com/questions/199686/fork-vs-clone-on-2-6-kernel-linux
          - http://man7.org/linux/man-pages/man2/clone.2.html
        Answer:
          - Hint: separate address space or not.
          - Solution: |
              By using fork(), the calling process creates a copy of itself. The new child process contains a separate copy of the parent's
              address space. Whereas, the process created by clone() can share some of the data of the parent process. The user has control
              over what the parent and the child share, like the table of signal handlers, virtual address spaces, and file descriptors.
              Now, the question remains that which is better. The answer depends on our use. The fork call is portable because the child
              the process is separate from the parent, whereas the clone call is not.
              Note: fork uses a clone system call internally, and clone is also used when we use pthread_create() for creating new threads.
      - Question_12: What are signals? What is their use? How are they used?
        Company_tags:
        Level:
        Reference:
          - https://en.wikipedia.org/wiki/Signal_(IPC)
          - https://www.usna.edu/Users/cs/aviv/classes/ic221/s16/lec/19/lec.html
        Answer:
          - Hint: A form of Inter Process Communication (IPC).
          - Solution: |
              Signal are software interrupts that are used by processes and threads to communicate with each other. They can be used in many
              ways. E.g., to tell if a particular task is completed, kill a process/thread, if an exception occurs, build user-defined
              procedures, communicate between processes, etc. Signals are used for IPC.
              Threads can be used to:
              - Same signal to all the threads.
              - Same signals to some of the thread and some other signals to others.
              - A signal only for one specific thread.
              - All signals to a specific thread.
      - Question_13: Is keeping the parameters for the thread function an advantage? If yes, explain why.
        Company_tags:
        Level:
        Reference:
        Answer:
          - Hint: Different types of parameters
          - Solution: |
              When we create a thread function, we declare it as "void *function_thread(void *parameters)". The return value and the parameter
              type are kept as void pointers. It is for the convenience because the thread function can act as a generic function for any data
              type. For example, if we create a thread for adding two numbers in a process, we can typecast the integer or float or double
              pointer to a void type and pas it to the thread as a parameter. But, if we would have kept explicit types of the thread function,
              we will have to build three different functions. This is why the return types and the parameter type of the thread is "void*"
              type.
      - Question_14:
Scheduling:
  Resources:
    - https://www.tutorialspoint.com/operating_system/os_process_scheduling.htm
    - https://www.studytonight.com/operating-system/process-scheduling
    - https://www.guru99.com/cpu-scheduling-algorithms.html
    - https://www.studytonight.com/operating-system/cpu-scheduling-algorithms
  Practice_questions:
    - Question_1: Of the three types of schedulers present in an OS, which executes most frequently?
      Options:
        - 1: Short Term Scheduler
        - 2: Mid Term Scheduler
        - 3: Long Term Scheduler
      Company_tags:
      Level:
      Reference:
      Correct_answer: 1
      Answer:
        - Hint:
        - Solution: |
            Short Term Scheduler is executed most frequently because it is responsible for selecting the next process to be executed. It is
            also known as dispatcher.
    - Question_2: Long Term Scheduler controls the degree of multiprogramming. True or False?
      Company_tags:
      Level:
      Correct_answer: "True"
      Reference:
      Answer:
        - Hint:
        - Solution: |
            It is true because the long term scheduler is responsible for selecting a process to be brought into the ready queue. Now, if
            the long term scheduler is not called frequently enough, the ready queue may become empty, and thus there may not be multi-
            programming.
    - Question_3: What are the conditions under which a non-preemptive process leaves the CPU (i.e., a voluntary preemption occurs)?
      Company_tags:
      Level:
      Reference:
        - https://www.geeksforgeeks.org/preemptive-and-non-preemptive-scheduling/
      Answer:
        - Hint:
        - Solution: |
            - When the process reaches termination.
            - When the process voluntarily goes into the waiting state.
            Note: Except for these conditions, rest all conditions lead to non-voluntary preemption.
    - Question_4: What is the difference between a scheduler and a dispatcher?
      Company_tags:
      Level:
      Reference:
        - https://www.geeksforgeeks.org/difference-between-dispatcher-and-scheduler/
      Answer:
        - Hint:
        - Solution: |
            A dispatcher comes into action after a short-term scheduler selects a process that will get a chance to execute in the CPU. Thus,
            the dispatcher is a module that gives a process control over the CPU. It involves the following steps:
            - Switching context
            - Switching to user mode
            - Jumping to the proper location in the user program to restart that program.
    - Question_5: What are the Scheduling Criteria that we need to keep in mind while designing a scheduling algorithm?
      Company_tags:
      Level:
      Reference:
        - https://www.youtube.com/watch?v=bWHFY8-rL5I
      Answer:
        - Hint:
        - Solution: |
            - CPU Utilization: Making sure that the maximum of the CPU is utilized most of the time and that utilization is productive. As we
                               will see in deadlocks, the CPU may be executing a wait function indefinitely, and then though it will be busy,
                               it will not be productive.
            - Throughput: We need to be sure that the throughput of the system is maximum.
            - Turnaround Time: If a process is submitted, then the time till it's completion should be minimum.
            - Deadlines: A maximum number of processes must meet their deadlines (real-time processes).
            - Response Time: If a process requests something, then it must be granted that as soon as possible.
    - Question_6: |
        Convoy Effect is when a process with a large CPU burst is executed ahead of processes with short CPU bursts; it causes starvation in
        for the processes with shorter burst time. Will there be a "Convoy Effect" involved in (First-Come-First-Serve)FCFS scheduling?
        Assume that all the processes involved in this scheduling are CPU bound only, and there is no quantum time for any process.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            Yes. FCFS involves a "Convoy Effect" when the processes are strictly CPU bound. This is because if the process with longer CPU
            burst arrives first then it will not exit the CPU until it has finished its execution.
            For e.g., Let there be three processes P1, P2 & P3 with CPU execution time as 500 ns, 50ns & 60ns, respectively, they arrive in
            order P1, P2, P3, respectively. This will mean that if P1 arrives first, it will execute for 500ns, and then only P2 will
            execute and then P3. This will cause starvation for P2 and P3. Now, if P2 was admitted to the ready queue first, then P3 and
            then P1, there would not have been that much starvation. This shows that FCFS shows "Convoy Effect".
    - Question_7: What is the main disadvantage of the Shortest Job First (SJF) Scheduling Algorithm?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            What SJF algorithm does is takes the process with the least next CPU burst as the next process to be executed. But, as we see
            the main problem here is knowing what will be the next CPU burst of any process. We cannot predict this with 100 percent
            accuracy.
    - Question_8: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Using the scheduling algorithm "Shortest Remaining Time First", answer following the questions.
                      -------------------------------------------
                      | Process | Arrival Time | Execution Time |
                      -------------------------------------------
                      |    A    |       0      |       3        |
                      |    B    |       2      |       6        |
                      |    C    |       4      |       4        |
                      |    D    |       6      |       5        |
                      |    E    |       8      |       2        |
                      -------------------------------------------
        For example, the first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: B
        - 2: C
        - 3: D
        - 4: E
      Correct_answer: 4
      Answer:
        - Hint:
        - Solution: |
            As only process 'A' is available in the ready queue initially, the CPU will start executing it. At time = 2, process 'B' will
            enter the ready queue and that time process 'A' will have remaining time as 1 and process 'B' will have it as 6. So, process 'A'
            will continue executing. It will finish execution at time = 3 units. Only process 'B' exists in the ready queue at that time. So,
            it will start executing and at time '4' units, process 'C' will enter the ready queue. Now, process 'C' has an execution time of
            4 units and process B has a remaining time of 5 units. This means that process C has the shortest remaining time. Proecss starts
            executing and at time = 6, process D enters with time of execution as 5 and process C has a remaining time of 2 units and process
            B has a remaining time of 5 units. So, process C will continue to execute and finish at time = 8. At that time, we will have 3
            processes in the ready queue, B, D, and E. The process with the shortest remaining time is process E/ So it will finish its
            execution and then we have to processes with the same remaining time. It will then depend on the algoritm implementation to
            decide if process B executes or D. Let us assume that process B will execute as it was admitted before process D. This implies
            that the sequence of completion of execution is in the order: A,C,E,B,D.
    - Question_9: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Using the scheduling algorithm "Shortest Remaining Time First", answer following the questions.
                      ---------------------------------------------------------------------
                      | Process | Arrival Time | Execution Time | I/O Time |  Time to I/O |
                      ---------------------------------------------------------------------
                      |    A    |       0      |       7        |     5    |       3      |
                      |    B    |       2      |       9        |     2    |       5      |
                      |    C    |       4      |       6        |     0    |       6      |
                      |    D    |       6      |       8        |     1    |       4      |
                      ---------------------------------------------------------------------
        For example, the first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        I/O time zero means that it is a CPU bound process. Also, if process x has 'a' units of time to I/O, it means that it will execute
        for 'a' time units first and then go for I/O.
        What will the decision points here? Which process will finish its execution first? At what time will it do that? What is the main
        disadvantage of using this algorithm?
      Company_tags:
      Level:
      Answer:
        - Hint:
        - Solution: |
            The decision points here will be the arrival of a process and remaining burst time.
            As for the mian disadvantage, after solving the problem and drawing a gnatt chart, we can come to a conclusion that the number
            of decision points are too much. This means context switching will mostly be high in general scenarios. This will reduce the
            efficiency of the system.
            Now let's go through the scheduling. As process A will arrive first, it will be the first one to be executed. At time = 2 process
            B will arrive. Now, process A has 1 second left (of the time quantum) to execute and proess B has 5. So, process A will continue
            execution till time = 3. Then it will go for I/O and will retuirn at time = 8. At that time only process B will be available in
            the ready queue and will start executing. At time = 4, process C comes in ready queue. Now, process B has 4 units of time
            remaining before going to I/O which is less than what process C will execute for. So, process B will continue to execute until
            time = 6 because that is when process D enters. Now, as B has the least remaining time, i.e., 2 time units, it will execute and
            go for I/O and return at time = 10. Now comes the crucial point where process A returns and there are processes C and D already
            in the queue. Based on the remaining execution time, process A will execute till time = 10 as that is when process B return and
            this makes that instance a decision point. Still process A will continue to execute as it has the least remaining execution time
            and will go for I/O at time = 11 and return at time = 16. The process with least remaining execution time is B (9-5=4) and D.
            Let us assume that process D executes instead of B because this will depend on how tie breaker is decided in the algorithm
            implementation. Now, it will continue to execute till time = 15 and go for I/O and return at time = 16. As no other process is
            admitted to the ready queue or returns from I/O. Then process B will execute for 1 time unit (it being the process with least
            remaining time) as process A returns from I/O at time = 16. Process A being the one with the least remaining time (7-6=1) will
            execute and complete. Here, we need to understand that had process D executed instead of process B, process A would have been the
            first one to finish its execution completely and exit. instead, process B becomes the one to finish its execution first and exit.
            Now, process D will execute and go for I/O and come back after 1 time unit and then again execute for 3 time units and complete
            its execution and exit and then process C which got preempted by the arrival of process D D from I/O will execute for the
            remaining time as it is a CPU bound process and its total execution time is equal to its time quantum.
    - Question_10: What will the decision points for a non non-preemptive scheduling algorithm?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            The decision points will be time quantum expiry, going for I/O, termination (in which which case it will exit).
    - Question_11: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Using the non-preemptive version of Priority Scheduling, answer following the questions.
                      ------------------------------------------------------
                      | Process | Arrival Time | Execution Time | Priority |
                      ------------------------------------------------------
                      |    A    |       0      |       3        |     5    |
                      |    B    |       2      |       2        |     3    |
                      |    C    |       4      |       5        |     2    |
                      |    D    |       6      |       4        |     4    |
                      |    E    |       8      |       1        |     1    |
                      ------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        What will be the descision points in this algorithm? What will be the order of completion of the processes?
    - Question_12: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Instead, now use the preemptive version of Priority Scheduling to answer the following questions.
                      ------------------------------------------------------
                      | Process | Arrival Time | Execution Time | Priority |
                      ------------------------------------------------------
                      |    A    |       0      |       3        |     5    |
                      |    B    |       2      |       2        |     3    |
                      |    C    |       4      |       5        |     2    |
                      |    D    |       6      |       4        |     4    |
                      |    E    |       8      |       1        |     1    |
                      ------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        . Will the decision points still be the same? If not, what extra
        decision points do we have to take care of? Will the order of completion of processes change? Explain.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_13: Can we, techinically speaking, implement FIFO Scheduling using Round Robin?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_14: |
        Is there any relation between time quantum and the context switching time? Explain.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_15: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Use Virtual Round Robin to answer the following questions.
                      ---------------------------------------------------------------------
                      | Process | Arrival Time | Execution Time | I/O Time |  Time to I/O |
                      ---------------------------------------------------------------------
                      |    A    |       0      |       7        |     5    |       3      |
                      |    B    |       2      |       9        |     2    |       5      |
                      |    C    |       4      |       6        |     0    |       6      |
                      |    D    |       6      |       8        |     1    |       4      |
                      ---------------------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        I/O time zero means that it is a CPU bound process. Also, if process x has 'a' units of time to I/O, it means that it will execute
        for 'a' time units first and then go for I/O.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_16: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Use Priority Scheduling with Round Robin to answer the following questions.
                      ------------------------------------------------------
                      | Process | Arrival Time | Execution Time | Priority |
                      ------------------------------------------------------
                      |    A    |       0      |       4        |     4    |
                      |    B    |       2      |       5        |     3    |
                      |    C    |       4      |       8        |     2    |
                      |    D    |       6      |       7        |     1    |
                      |    E    |       8      |       3        |     4    |
                      ------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        The time quantum for Round Robin is 2 time units.
    - Question_17: |
        Assume that Round Robin Scheduling is used for scheduling processes. What will happen if a process forks after completion of half of
        its time quantum? What will be the time quantum for the child and the parent process? Will their remaining time quantum be related?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_20: What are the types of scheduling a process can be assigned to in a linux kernel? Explain
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_21: |
        When a parent process forks a child process, what is the relation of the priority of the child with that of its parent? Explain.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_22: Can we change the static priority of a process? If yes, how? If no, why?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_23: What is the relation between the nice value of a process and its static priority?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_24: What will be the dynamic priority of a process with a static priority of 120?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_25: How is the property "Bonus" of process related to its scheduling?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_26: |
        Is there a relation between base time quantum of a process and its priority? Is yes, what will the base time quantum of a process with
        static priority 121?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_27: |
        Will the average sleep time of a process which spend more time in CPU executing increase or decrease? Is there a relation between the
        term "Bonus" and average sleep time?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_28: Is a process with dynamic priority 130 an interactive process? Explain.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_29: |
        Context: Expired Array and Active Array are used by Linux Scheduler to schedule Real Time processes.
        Are the data structures used for maintaining an expired array and an active array same or different? If yes, what is the advantage
        of keeping them same?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution:
    - Question_30: |
        Given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Use Priority Scheduling with Round Robin to answer the following questions.
                      ---------------------------------------------------------------------
                      | Process |  Nice Value  | Execution      | I/O Time |  Time to I/O |

                      ---------------------------------------------------------------------
                      |    A    |       8      |       7        |     5    |       3      |
                      |    B    |      -15     |       9        |     2    |       5      |
                      |    C    |      -5      |       6        |     0    |       6      |
                      |    D    |       0      |       8        |     1    |       4      |
                      ---------------------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        The time quantum for Round Robin is 2 time units.
Miscellaneous:
  - Practice_questions:
      - Question_1: |
            S1: If an entry is not found in the page table, the page table is not invalid. Rather, it is fetched because loading a Page
                Table is a dynamic job.
            S2: The frame size in a frame table can be 4KB or 4MB in a 32-bit system.
        Options:
          -1: TRUE/TRUE
          -2: TRUE/FALSE
          -3: FALSE/TRUE
          -4: FALSE/FALSE
        Reference:
          - https://cs.stackexchange.com/questions/47541/is-page-size-always-equal-to-frame-size
        Company_tags:
        Level:
        Correct_answer: 1
        Answer:
          - Hint:
          - Solution: |
              If the frame entry is not found in a page table, we so to swap space and bring back the entry. But any entry that is beyond the
              Page Table Limit Register is invalid. This type of loading is done because it is dynamic in nature. The frame size depends on
              the implementation of the architecture. It is generally 4KB or 4MB. But, you can modify it to what you want.

      - Question_2: |
          What is the effective access time of memory when there is an 80% hit in TLB and the access time for TLB is 20ns and that of the
          memory is 100ns?
        Company_tags:
        Level:
        Options:
          -1: 140ns
          -2: 150ns
          -3: 80ns
          -4: 160ns
        Correct_answer:
        Answer:
          - Hint:
          - Solution: |
              The correct answer is option b. Hit percentage means out of 100, how many memory accesses were present in TLB. For example,
              if the hit percentage is 80%, 60% of the time, we find what we need to access and the other 20% of the time, we do not and hence
              there is an exception, and the CPU takes over, loads that memory page location into the TLB, and again executes the instruction.
              Therefore, 80% hit means 80% successful accesses but, out of 80%, the first 60 are successful the first time and the remaining
              20 the second time, after the CPU loads them into the TLB. No, that we are clear with hit percentage, the question is pretty simple.
              Effective access time = ((hit%*(TLB access time + Memory access time))  + ((1-hit%)*(Memory access time + TLB access time)))/hit%
              = (0.8*(100+20) + (100+20))/0.8 = 150ns.
              Now we have multiplied (1-hit%) with 120 because, once it will search the TLB for the page location, then when it will not find it,
              it will access the memory and load it into TLB.
      - Question_3: >
          S1: System Library (libc.so) is linked at compile-time, and Shared Library (libm.so) is linked dynamically.
          S2: If a process is swapped, it can return to any address at swapping back to the main memory.
          Please select the correct option, where “TRUE/TRUE” indicates that S1 is TRUE, and S2 is TRUE.
        Company_tags:
        Level:
        Options:
          -1: TRUE/TRUE
          -2: TRUE/FALSE
          -3: FALSE/TRUE
          -4: FALSE/FALSE
          Correct_answer: 2
          Answer:
            - Hint:
            - Solution: |
                The statement S1 is true and is a fact. The second statement is not always true. The swapped out process must return to the same
                address if they had binding that was done compile time or load time. This is the reason why we share a driver file and not
                the compiled file.
      - Question_4: Which of the following will guarantee a deadlock in a single instance resource?
        Company_tags: abc
        Level: xyz
        Options:
          -1: Mutual Exclusion
          -2: Circular Wait for graph
          -3: Hold and Wait
          -4: Preemption of one of the process.
        Correct_answer: 2
        Answer:
          - Hint: Single instance resource and circular wait are related.
          - Solution: |
              The correct option is b. This is because if every process in the graph is holding a resource and requesting for other and another
              process is requesting for the resource, the first is holding there is a definite deadlock. The others do not guarantee that.
      - Question_5: Trap Bit question.
        Company_tags: abc
        Level: xyz
        Reference:
          - https://stackoverflow.com/questions/13185300/where-is-the-mode-bit
          - https://stackoverflow.com/questions/28815848/does-each-core-has-its-own-private-set-of-registers
          - http://www.cs.cornell.edu/courses/cs3410/2012sp/lecture/22-traps-i-g.pdf
        Answer:
          - Hint:
          - Solution: #Will write soon
  - Premium/Quiz_questions:
      - Question_1: >
          A user-space process crash can be handled gracefully by exception handling. Is it possible to
          gracefully handle a crash in kernel driver for a monolithic kernel like LINUX?
        Company_tags:
        Level:
        Reference:
          - https://stackoverflow.com/questions/32644692/why-kernel-cant-handle-crash-gracefully
          - Chapter 8, Topic - Kernel Basics, Mac OS X and iOS Internals, Jonathan Levin
        Comment:
        Options:
          - No, a crash in kernel driver would simply crash the whole kernel leading to kernel panic
          - Yes, we can continue running the kernel. Simply crashing the faulty kernel driver is enough
          - Yes, we can continue running the kernel. We need to crash faulty driver and all its associated entities
          - None of these
        Correct_answer: 1
        Answers:
          - Hint: For monolithic kernels like LINUX, the kernel module and the kernel itself share the same address space.
          - Solution: |
              LINUX uses a monolithic kernel architecture. A monolithic kernel is an operating system architecture where all
              the operating system components(including the device drivers, file system, and the application IPC) are working
              in a single kernel address space. Due to such type of design, there is no protection boundary between modules. If a
              kernel module starts misbehaving, it can overwrite memory from another subsystem. So, when a driver crashes,
              it may or may not stay local to that driver. It is not possible to know if it has poisoned the kernel memory.
              Security is the primary reason behind the design of crashing the entire kernel on the occurrence of a kernel driver crash.
              This is not the case with userspace processes. This is because the address space for each process is separate
              Thus, it is possible to catch erroneous memory access(or any other fault) and terminate the process.
              Microkernels(in which services have separate address space) can overcome this situation. Whereas,
              Hybrid kernels(Windows and macOS) also face this issue due to their adoption/essence of monolithic kernel design.
