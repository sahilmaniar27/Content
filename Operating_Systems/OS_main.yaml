---
Subject:
  name: Operating Systems
  subtopics:
    - Introduction
    - Process & Threads
    - Scheduling
    - Memory & Address Space
    - Virtual memory & paging
    - Concurrency
    - Synchronization
    - Persistence
    - Miscellaneous
  name: DSTN
  subtopics:
    - Memory Management
    - File System
Process & Threads:
  - Process:
    Concept of Process and Process Attributes:
      References:
        - Gate Lectures by Ravindrababu Ravula: https://youtu.be/ucVm_arB-fw
        - Neso Academy: https://youtu.be/OrM7nZcxXZU
        - Operating System Concepts Peter B. Galvin: Chapter 3, section 3.1
      Practice_questions:
        - Question: |
            State true or false.
            A process can be in multiple states at a time.
          Company_tags: None
          Level:
          Options:
            - 1: "True"
            - 2: "False"
          Reference:
          Correct_answer: 2
          Answers:
            - Hint: A process can only exist in 1 state at a time.
            - Solution: A process can only be in one state at a time. As in, it can be in a ready state or a blocked state or swapped state, etc.
        - Question: |
            A file is opened by the user. What will be the changes in the Local Descriptor Table(LDT) and Global Descriptor Table(GDT)?
            Moreover, if two different processes have opened one instance each of a file and one of the processes close it, what will be the
            changes in the LDT and GDT?
          Company_tags: None
          Level:
          Reference:
          Answer:
            Hint: GDT has an entry, even if one instance is there.
            Solution: |
                LDT is also called a per-process descriptor table, and GDT is called a system-wide descriptor table. When you initially open a
                file, both LDT and GDT make new entries of the file. However, if it is already opened by even one process, the GDT only increases
                the counter. If an instance of a file opened by another process and a new process opens it, a new instance is made in LDT.
                When a file is closed by one of the processes, given that only a single instance is opened by each process, only the
                counter in GDT is decreased. However, in LDT, the entry is deleted (as only one instance was opened, and that too was
                closed). There is an entry in the GDT for a file as long as there is at least one instance of the file opened by a process.
        - Question: |
            Which of the following is correct about Page Table?
            S1: It keeps track of the main memory associated with the process.
            S2: It keeps track of secondary memory to processes.
            S3: It keeps the information needed to manage Physical Memory.
            S4: It keeps track of protection attributes of main and secondary memory.
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.javatpoint.com/os-page-table
          Options:
            - 1: S1
            - 2: S2
            - 3: S3
            - 4: S4
          Correct_answer: S3
          Answer:
            - Hint: The OS only generates Logical Address.
            - Solution: |
                S3 is wrong because OS only generates Logical Address, so Memory Table, also known as a page table, keeps track of virtual
                memory of the process. Note that user programs never see physical addresses. User programs work entirely in logical
                address space, and any memory references or manipulations are done using purely logical addresses. Only when the address
                gets sent to the physical memory chips is the physical memory address generated.

        - Question: Which of the following attributes of a thread are private to itself?
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.geeksforgeeks.org/thread-in-operating-system/
          Options:
            - 1: Heap Space
            - 2: Stack Space
            - 3: File Descriptors
            - 4: Signal Handler
          Correct_answer: 2
          Answer:
            - Hint:
            - Solution: |
                A thread shares with its peer threads its Code section Global data section, Operating-system resources, Process instructions,
                Open files (descriptors), Signal and signal handlers, Current working directory, and User and group id. But, Stack space,
                Signal Masks and Register Set is private for a thread. It also has a separate return value for itself (errno).

        - Question: Which of the following is an indirect way of communication between processes (IPC)?
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/8_MainMemory.html (It's section 8.1.3)
          Options:
            - 1: Shared Memory
            - 2: Signals
            - 3: Mailbox
            - 4: Passing Pointers
          Correct_answer: 3
          Answer:
            - Hint:
            - Solution: |
                The correct option is c. We pass signals explicitly to the processes we need to communicate to, and shared memory is with the
                passing pointers. Mailbox is called a port. It consists of a queue of messages. The sender keeps the message in the processes
                we want to share it with (processes need to join themselves to the shared memory block). Similar is the passing pointers.
                Mailbox is called a port. It consists of a queue of messages. The sender keeps the message in the
                mailbox, and the receiver picks them up.
    Fork and Exec system calls:
      References:
        - Neso Academy (YouTube): https://www.youtube.com/watch?v=IFEFVXvjiHY
        - Techtud (YouTube): https://www.youtube.com/watch?v=PwxTbksJ2fo
        - Operating System Concepts by Abraham Silberschatz, Peter B. Galvin, and Greg Gagne: Chapter 3, Section 3.3
        - GeeksForGeeks: https://www.geeksforgeeks.org/difference-fork-exec/
      Practice_questions:
        - Question: How is PID assigned to a process?
          Company_tags: None
          Level:
          Reference:
          Options:
            - 1: Randomly
            - 2: It is the next vacant PID
            - 3: It is the next vacant PID from the start
            - 4: It is the least vacant PID
          Correct_answer: 2
          Answer:
            - Hint: Go through how PID is assigned
            - Solution: |
                PID is always assigned as the next vacant PID. For ex. if the current PID is x, the next PID will be the next vacant PID slot after x.
                It may be x+1, x+2 or even 3, 4, etc. It is determined by a circular search.
        - Question: |
            Read the following piece of code carefully.
            int main(){
              printf("Hello, I am in main process");
              char* args = "Hello World\0";
              execv("./prog1",args);
              printf("Execv exected. Exiting main process");
            }
            Will the second print statement be executed?
          Company_tags: None
          Level:
          Reference:
            - 1: https://linuxhint.com/linux-exec-system-call/
          Answer:
            - Hint: Exec repalces the process that calls it.
            - Solution: |
                No. the second print statement will not be printed. It is because the exec() system call replaces the process that calls it.
                So, when exec() will be executed, it will execute the code that it's .c file contains. The main process gets replaced so it
                does not exist from the moment the exec() system call starts executing.
                Also remember, as the process created by exec() is assigned the PID of the calling process, it also replaces the address space
                of the calling process. It is the reason why we say that the original process does not exist after the exec system call.
        - Question: |
            If a process has PID as 'x', suppose it executes exec() at some pint of time.
            What will be the PID of the process (created due to the exec() call)?
          Company_tags: None
          Level:
          Reference:
          Options:
            - 1: x
            - 2: anything greater than x
            - 3: anything less than x
            - 4: Can't say
          Correct_answer: 1
          Answer:
            - Hint: exec() replaces the calling process.
            - Solution: |
                The PID of the new process created due to exec() will be 'x' itself. The explanation is simply that the exec() process replaces
                the original calling process. So, if you execute a print statement that will print the PID of the process, both will print 'x'.
        - Question: |
            If process P1 creates two processes P2 and P3. Who will be the previous sibling of P2 and
            who will be the next sibling of P1, respectively?
          Company_tags: None
          Level:
          Reference:
          Options:
            - 1: P1 and P2 respectively
            - 2: P1 and P2 respectively
            - 3: P3 and P3 respectively
            - 4: P2 and P1 respectively
          Correct_answer: 2
          Answer:
            - Hint:
            - Solution: The next sibling of the parent is the last child it creates, and the previous sibling of the 1st child is the parent itself.
        - Question: What is the return value of fork()?
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.geeksforgeeks.org/fork-system-call/
          Answer:
            - Hint: There are two.
            - Solution: >
                It returns the PID of the child to the parent process (i.e., if the fork() is successful), else it returns '-1'.
                As for the second return value, it returns '0' to the child that is created.
        - Question: |
            Let there be two concurrent processes, P1 and P2.
              P1 -> {
                //some code
                x++;
                //remaining code
              }
              P2 -> {
                //some code
                x--;
                //remaining code
              }
            Let the initial value of x be zero (0). What will be minimum value of “x” after these processes complete their execution?
            (Assume the “some code” and “remaining code” do not modify x)
          Company_tags: None
          Level:
          Reference:
          Option:
            - 1: 0
            - 2: 1
            - 3: 2
            - 4: -1
          Correct_answer: 4
          Answer:
            - Hint:
            - Solution: |
                The possibility of context switch at any time during the execution of x++ or x-- is the reason that makes this possible.
                “x--” essentially means,
                load 'x' in a reg; ---(1)
                Reduce the 'reg' by '1'; ---(2)
                store 'x' in the 'reg';  ---(3)
                Similar is the case with “x++". Now if there is a context switch after (1), then if “x++” executes and then (2) and (3)
                executes, the value in “reg” will be zero before context switch, and after executing “x++” also, “reg” will contain zero.
                And hence after stages 2 and 3, the final value of x will be “-1”. Similarly, the max value of x can be 1.
        - Question: |
            Find the total number of new processes created. Draw a process tree.
            Assume that the necessary libraries are included and that the fork() call produces child processes and does not give '-1'
            (which is return when fork does not happen successfully).
            int main()
            {
              printf("fork program starting with pid %d\n",getpid());
              if(fork())
                if(!fork())
                  fork();
              printf("My PID=%d My PPID=%d\n",getpid(),getppid());
              wait(NULL);
              wait(NULL);
              return 0;
            }
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.geeksforgeeks.org/creating-multiple-process-using-fork/
            - 2: https://linuxtrainers.wordpress.com/2014/12/31/how-fork-system-call-works-what-is-shared-between-parent-and-child-process/
            - 3: https://unix.stackexchange.com/questions/87551/which-file-in-kernel-specifies-fork-vfork-to-use-sys-clone-system-call
          Correct_answer: 3
          Answer:
            - Hint:
            - Solution: |
                Please go through the reference material for this question first before reading the solution.
                Now, for the solution part, remember that the return value for the fork is 'zero' for the child and 'the PID of the child' to the
                parent.
                When the parent process executes the fork() call, it will create a new process. The PID of the process will be the
                return value in the 'if condition' of the parent process, so it will go to the 'inner if condition'. However, as for the 1st child
                process, the value will be 'zero' in its 'if condition', this is because the return value of fork is 'zero for the child.
                Then, the child begins the execution at the same point where the fork() call returns execution to the main program. However,
                because it does not satisfy the 1st 'if condition', it goes on to print and exit. Then, the parent forks one more process.
                The child that is created will move ahead in the 'if condition' tree as '!0' evaluates 'true' for the inner 'if condition'.
                So, it goes inside that and executes one more fork(). As for the parent, it does not go ahead in the 'if condition' as '!PID'
                evaluates as false. Note that PID zero is permanently assigned to the 'swapper process' or 'sched process'. So, '!PID' will
                always always be 'false'. So, it will wait for the two children to finish execution (two because of two wait() calls). In
                this way, there will be three new processes created.
                As for the process tree, it will be like this:
                                        P1 -----> Parent Process
                                      /  \
                                     /    \
                      1st child<----P2     P3---->2nd child
                                            \
                                             P4----->1st child of 2nd child
        - Question: |
            What will be the value of 'x' in each print statement? Assume all processes will be created.
            int x=0;
            int main()
            {
              if(fork())
                x=x+5;
              else if(!fork())
              {
                x=x+10;
                printf("PID = %d X=%d\n",getpid(),x);
                exit(0);
              }
              else
                x=x-2;
              x=x+15;
              printf("PID=%d and X =%d\n",getpid(),x);
              while(wait(NULL)!=-1);
                return 0;
            }
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.geeksforgeeks.org/fork-memory-shared-bw-processes-created-using/
          Answer:
            - Hint:
            - Solution: |
                The parent process forks a child process and will go inside the if condition. It will first increase 'x' by '5' and then by '15' and
                then wait till all of its children to die (because of the while loop). The 1st child will not go inside the 'if condition'
                and will go to the 'else if' statement. But, it will not go inside the else if statement because of '!fork()' condition in
                the 'else if' statement. At the same time, due to a successful fork, the 1st child will create a child of its own, and
                that child will enter the 'else if' statement. Concurrently, the 1st child of the parent will keep on going forward. So,
                it will execute the else condition and decrease the value of 'x' by '2' and then increase it by '15'. This child did not have
                the value of 'x' to be '5' initially because when it was created, the value of 'x' was 'zero'.The reason
                being, the child, contains a copy of all variables the parent had at the time of forking (except for shared
                variables, which are the same for both parent and child). Then, the 1st child of the parent process will also wait for its
                child to exit. The child of the 1st child will execute the statements inside of the 'else if' statement and exit with status
                '0'. The value of 'x' in this process (child of the child of the parent process) will thus be '0+10',i.e., '10'.
                So, the value of 'x' in parent process will be '0+5+15'='20' and that in 1st child will be '0-2+15'='13'. Here is the process
                tree:
                                                        P1-----> Parent process, x=20
                                                       /
                                                      P2------>1st child, x=13
                                                     /
                                                    P3------>child of the child of parent, x=10
        - Question: Why can't a forked process have a PID as 0 or 1?
          Company_tags: None
          Level:
          Reference:
            - 1: https://unix.stackexchange.com/questions/83322/which-process-has-pid-0
            - 2: https://hackernoon.com/the-curious-case-of-pid-namespaces-1ce86b6bc900
          Answer:
            - Hint:
            - Solution: |
                A process can have a PID between 0 and 32767. The upper range can be found in a system's '/proc/sys/kernel/pid_max' file.
                It is generally '32767'. But, the process that is created using fork cannot be assigned a PID '0' or '1'. It is because
                the PID '0' is given to the first process that the OS creates, and this process is generally the 'swapper process' or 'sched
                process'. This process is responsible for the paging in the system. This process dies or exits when the OS shuts down. So,
                this PID is never vacant. Then, PID '1' is the 'init process'. When this process dies, all the processes are killed with the
                kill() signal. So, this is why a forked process cannot have a PID '0' or '1'.
        - Question: |
            A user-space process crash can be handled gracefully by exception handling. Is it possible to
            gracefully handle a crash in kernel driver for a monolithic kernel like LINUX?
          Company_tags:
          Level:
          Reference:
            - 1: https://stackoverflow.com/questions/32644692/why-kernel-cant-handle-crash-gracefully
            - 2: Chapter 8, Topic - Kernel Basics, Mac OS X and iOS Internals, Jonathan Levin
          Comment:
          Options:
            - 1: No, a crash in kernel driver would simply crash the whole kernel leading to kernel panic
            - 2: Yes, we can continue running the kernel. Simply crashing the faulty kernel driver is enough
            - 3: Yes, we can continue running the kernel. We need to crash faulty driver and all its associated entities
            - 4: None of these
          Correct_answer: 1
          Answers:
            - Hint: For monolithic kernels like LINUX, the kernel module and the kernel itself share the same address space.
            - Solution: |
                LINUX uses a monolithic kernel architecture. A monolithic kernel is an operating system architecture where all
                the operating system components(including the device drivers, file system, and the application IPC) are working
                in a single kernel address space. Due to such type of design, there is no protection boundary between modules. If a
                kernel module starts misbehaving, it can overwrite memory from another subsystem. So, when a driver crashes,
                it may or may not stay local to that driver. It is not possible to know if it has poisoned the kernel memory.
                Security is the primary reason behind the design of crashing the entire kernel on the occurrence of a kernel driver crash.
                This is not the case with userspace processes. This is because the address space for each process is separate
                Thus, it is possible to catch erroneous memory access(or any other fault) and terminate the process.
                Microkernels(in which services have separate address space) can overcome this situation. Whereas,
                Hybrid kernels(Windows and macOS) also face this issue due to their adoption/essence of monolithic kernel design.
        - Question: Explain in brief what happens during the fork() system call.
          Company_tags: None
          Level:
          Reference:
          Answer:
            - Hint:
            - Solution: |
                The following steps take place when a fork() executes:
                - A slot is allocated in the process table for the new process.
                - A unique process id is assigned to the new process.
                - A copy of the process image of the parent is made, except for shared memory.
                - fork() also increases counters for any files owned by the parent, to reflect that an additional process now also owns
                  these files.
                - The kernel (during the fork() call) also assigns the child process to a ready state.
                - A Process ID number (PID) of the child to the parent process and a 0 value to the child process is returned.
                - After completing these functions, OS will do the following operations as a part of the dispatcher routine:
                  • Control returns to the user mode at the point of the fork call of the parent
                  • Transfer control to the child process. The child process begins executing at the same point in the code as the parent,
                  namely at the return from the fork call, i.e., the instruction next in line after fork().
                  • Transfer control to another process. Both child and parent are left in the ready state.
        - Question: What is the difference between fork() and vfork()?
          Company_tags: None
          Level:
          Reference:
            - 1: https://www.quora.com/What-is-the-difference-between-fork-and-vfork
          Answer:
            - Hint:
            - Solution: |
                • vfork() is a variant of fork(), and it behaves exactly like fork() except that it does not make a copy of the address
                  space and page table of the parent process.
                • Memory, including stack space, is shared by the parent and the child process.
                • vfork() suspends the parent process until the child releases its memory address space, i.e., until the child makes a call
                  to exec() or _exit().
        - Question: Explain Copy-On-Write (CoW) mechanism.
          Company_tags: None
          Level:
          Reference:
            - 1: https://unix.stackexchange.com/questions/58145/how-does-copy-on-write-in-fork-handle-multiple-fork
            - 2: https://www.reddit.com/r/compsci/comments/31szui/trying_to_understand_fork_and_copyonwrite_cow/
          Answer:
            - Hint: Lazy approach.
            - Solution: |
                Today's systems run with hundreds and thousands of processes. If the OS goes according to the rules and copies the memory
                in use by the parent process after every fork(), it will be very time-consuming because of the context switching involved. So,
                to avoid this, the OS waits until any one of the parent or child modifies the memory. When any one of them does, it then makes
                the two copies of memory it was supposed to do at the beginning of the fork().
                Additionally, as processes reside in virtual memory, whenever the child wants to read something, it reads from the parent
                provided; it has not written since it was created. But, as soon as it writes for the first time, the
                required copy of the parent memory is made (as it should have been at the start of the fork() call), and the child's memory
                space is updated. Then the child process writes to the newly created memory space. This is CoW.
        - Question: Explain the difference between wait() and waipid(). What are the different parameters of the two functions?
          Company_tags: None
          Level:
          Reference:
            - 1: http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch08lev1sec6.html
            - 2: https://webdocs.cs.ualberta.ca/~tony/C379/C379Labs/Lab3/wait.html
          Answer:
            - Hint:
            - Solution: |
                wait() waits for a child to terminate. It will be temporarily blocked till then. Whereas, waitpid can be either blocking or
                non-blocking:
                - If the "options" parameter is 0, then it is blocking.
                - If the "options" parameter is WNOHANG, then is it non-blocking.
                WNOHANG is one of the options of the 'options' parameter of the waitpid function.
                If more than one child is running, then wait() returns when the first time one of the child exits. Whereas, waitpid() is more
                versatile. Its 'pid' parameter provides that versatility. If:
                - pid < -1 means that it has to wait for any child process whose process group ID is equal to the absolute value of pid to
                  exit.
                - pid = -1 means that it has to wait for any child process to exit. At this point, it is equivalent to wait().
                - pid = 0 means that it has to wait for any child process whose process group ID is equal to that of the calling process.
                - pid > 0 means that it has to wait for the child whose process ID is equal to the value of pid.
                In this way, it is different from the wait() function.
        - Question: What is the difference between the 'exec' functions suffixed with 'p' and 'e'?
          Company_tags: None
          Level:
          Reference:
          Answer:
            - Hint: Need of specifying path variable.
            - Solution: |
                The 'exec' functions, which end with the suffix 'p' will search the current PATH variable to find the executable file. Whereas,
                the 'exec' functions suffixed with 'e' have to be given the PATH variable as an input parameter, and then it will find the
                file to be executed in that PATH variable.
                Note: PATH variable is the address of the file in terms of directories. For example, a file can have its PATH variable as
                      "/home/mypc/Downloads/myfolder"
      Premium/Quiz_questions:
        - Question: >
            Consider a process with multiple threads.
            What would happen if one of the threads crashes due to fault?
          Company_tags:
          Level:
          Reference:
            - 1: https://stackoverflow.com/questions/36423511/what-happen-if-thread-crashes-which-is-better-thread-or-process
          Comment:
          Options:
            - 1: Only, Thread with the fault crashes
            - 2: Entire process crashes
            - 3: Undefined behavior
            - 4: Cannot be predicted, either of the options can happen
          Correct_answer: 2
          Answers:
            - Hint: Threads share their address space.
            - Answer: |
                If one thread crashes due to a segmentation fault or any other error, all other threads and the entire process are killed.
                This is because the threads are part of the same process space, so it does not generally make any sense to keep going if a
                thread has caused a fault signal. A crash signal (like SIGSEGV, SIGBUS, SIGABRT) means that you have lost control over
                the behavior of the process, and anything could have happened to its memory. There is a negative side of the entire process being
                killed due to a fault in one of the threads. This situation could lead to data and system corruption if the other threads were
                in the middle of some important task. Due to this reason, important & independent tasks are generally performed as a separate
                process, such that the crashing of one does not bring down the entire functionality.
  - Thread:
    References:
      - GeeksForGeeks: https://www.geeksforgeeks.org/thread-in-operating-system/
      - Tutorials Point: https://www.tutorialspoint.com/operating_system/os_multi_threading.htm
      - Neso Academy: https://www.youtube.com/watch?v=LOfGJcVnvAk
    Practice_questions:
      - Question: What is the difference between User-level threads & kernel-level threads?
        Company_tags: None
        Level:
        Reference:
          - 1: www.cs.iit.edu/~cs561/cs450/ChilkuriDineshThreads/dinesh's files/User and Kernel Level Threads.html
        Correct_answer: None
        Answers:
          - Hint: None
          - Solution: |
              The user-level threads are created by the user. They reside in the user space and are managed by APIs. The APIs manage all
              the function calls made by these threads without invoking the kernel mode. Additionally, they are also invisible to the OS.
              Kernel-Level threads, as the name suggests, are created and managed by the kernel. The kernel knows everything about the threads.
              However, they are slower than the user level threads because the kernel manages and schedules threads as well as processes. Thus, it
              requires a full thread control block (TCB) for each thread to maintain information about a specific thread. As a result, there is
              significant overhead and an increase in kernel complexity.

      - Question: >
          Consider a process with multiple threads.
          What would happen if one of the threads crashes due to fault?
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/36423511/what-happen-if-thread-crashes-which-is-better-thread-or-process
        Comment:
        Options:
          - 1: Only, Thread with the fault crashes
          - 2: Entire process crashes
          - 3: Undefined behavior
          - 4: Cannot be predicted, either of the options can happen
        Correct_answer: 2
        Answers:
          - Hint: Threads share their address space.
          - Solution: |
              If one thread crashes due to a segmentation fault or any other error, all other threads and the entire process are killed.
              This is because the threads are part of the same process space, so it does not generally make any sense to keep going if a
              thread has caused a fault signal. A crash signal (like SIGSEGV, SIGBUS, SIGABRT) means that you have lost control over
              the behavior of the process, and anything could have happened to its memory. There is a negative side of the entire process being
              killed due to a fault in one of the threads. This situation could lead to data and system corruption if the other threads were
              in the middle of some important task. Due to this reason, important & independent tasks are generally performed as a separate
              process, such that the crashing of one does not bring down the entire functionality.
      - Question: What are the benefits of using a thread, instead of using a process?
        Company_tags:
        Level:
        Reference:
          - 1: https://www.tutorialspoint.com/operating_system/os_multi_threading.htm
        Answers:
          - Hint: Lightweight, easy to manage, etc.
          - Solution: |
              - Threads share memory & other resources of the parent process.
              - Threads are more economical to context switch.
              - Threads can run in parallel, and hence multiple tasks can be done parallelly.
              - Threads are also easy to create, switch between, and terminate.
      - Question: What is the difference between Data and Task Parallelism?
        Company_tags:
        Level:
        Reference:
          - 1: https://en.wikipedia.org/wiki/Task_parallelism
        Answer:
          - Hint:
          - Solution: |
              Data Parallelism is distributing data to and running them through the same task (thread/process). Task Parallelism is running
              different tasks on different cores on the same data.
              Both these parallelisms can be visualized as follows:
              - Data Parallelism------------>
                                                    -------------------DATA-----------------
                                                  /                                          \
                                                  --------------------------------------------
                                                 |    D0    |    D1    |    D2    |    D4     |
                                                  --------------------------------------------
                                                    |             |          |          |
                                                  Core-1        Core-2     Core-3     Core-4

              - Task Parallelism------------>
                                                    -------------------DATA-----------------
                                                  /                                          \
                                                  --------------------------------------------
                                                 |                                            |
                                                  --------------------------------------------
                                                    /             /             \             \
                                                  Core-1        Core-2         Core-3       Core-4

      - Question: Why does the return value of a thread function be of the type "(void *)" only?
        Company_tags:
        Level:
        Reference:
          - 1: https://en.wikipedia.org/wiki/Task_parallelism
        Answer:
          - Hint:
          - Solution: |
              #Will write soon
      - Question: |
          What does the "getpid()" function return when executed in a thread? Does it give an error? If not, what is the return value?
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/9305992/if-threads-share-the-same-pid-how-can-they-be-identified
        Answer:
          - Hint: Not an error.
          - Solution: |
              No, It will not give an error. By calling the function "getpid()" inside a thread, you will get the PID of the thread that created
              it. If we execute that function in a newly created thread (by a process), you will get the PID of the process that created it. So,
              executing the "getpid()" function in a thread will give you the PID of the process that created it.
              Note: We can think of a thread as a process that shares its address space with its parent process.
      - Question: >
          Like a process has a PID, does a thread have something similar to PID that the LINUX system uses to identify it(that thread)?
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/21091000/how-to-get-thread-id-of-a-pthread-in-linux-c-program
        Answer:
          - Hint: Using a System Call.
          - Solution: |
              Yes, there is a TID of a thread. We can get it by executing the system call "syscall(__NR_gettid)" in the thread (whose TID we
              want to know).
      - Question: If we create a thread and the return value is '0', does it mean that the creation has failed? Explain.
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/21091000/how-to-get-thread-id-of-a-pthread-in-linux-c-program
        Answer:
          - Hint: Not failed.
          - Solution: |
              No, getting '0' as a return value does not mean that the thread creation has failed. When we create a thread using the
              "pthread_create()" function, the kernel stores the identifier of the newly created thread in the location pointed by the argument
              in the function. Furthermore, if the creation is successful, a '0' is returned.
      - Question: What is the "detachable state" of a thread?
        Company_tags:
        Level:
        Reference:
        Answer:
          - Hint: Release of the holding resources.
          - Solution: |
              Detachable thread means that when it terminates, it will release all the resources that it is was allocated. A thread is in
              "Joinable State" by default. If we want to make the thread detachable, we need to make it explicitly using the "pthread_detach()"
              function and passing the thread as the argument.
      - Question: What happens if a thread executes the fork() call?
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/39890363/what-happens-when-a-thread-forks
          - 1: http://gauss.ececs.uc.edu/Courses/c694/lectures/ForksThreads/forks.html
        Answer:
          - Hint:
          - Solution: |
              A new process will be created, and the parent process will the process that created the thread that executed the fork call.
      - Question: Is fork call better than clone? Justify.
        Company_tags:
        Level:
        Reference:
          - 1: https://unix.stackexchange.com/questions/199686/fork-vs-clone-on-2-6-kernel-linux
          - 1: http://man7.org/linux/man-pages/man2/clone.2.html
        Answer:
          - Hint: separate address space or not.
          - Solution: |
              By using fork(), the calling process creates a copy of itself. The new child process contains a separate copy of the parent's
              address space. Whereas, the process created by clone() can share some of the data of the parent process. The user has control
              over what the parent and the child share, like the table of signal handlers, virtual address spaces, and file descriptors.
              Now, the question remains that which is better. The answer depends on our use. The fork call is portable because the child
              the process is separate from the parent, whereas the clone call is not.
              Note: fork uses a clone system call internally, and clone is also used when we use pthread_create() for creating new threads.
      - Question: What are signals? What is their use? How are they used?
        Company_tags:
        Level:
        Reference:
          - 1: https://en.wikipedia.org/wiki/Signal_(IPC)
          - 2: https://www.usna.edu/Users/cs/aviv/classes/ic221/s16/lec/19/lec.html
        Answer:
          - Hint: A form of Inter Process Communication (IPC).
          - Solution: |
              Signal are software interrupts that are used by processes and threads to communicate with each other. They can be used in many
              ways. E.g., to tell if a particular task is completed, kill a process/thread, if an exception occurs, build user-defined
              procedures, communicate between processes, etc. Signals are used for IPC.
              Threads can be used to:
              - Same signal to all the threads.
              - Same signals to some of the thread and some other signals to others.
              - A signal only for one specific thread.
              - All signals to a specific thread.
      - Question: Is keeping the parameters for the thread function an advantage? If yes, explain why.
        Company_tags:
        Level:
        Reference:
        Answer:
          - Hint: Different types of parameters
          - Solution: |
              When we create a thread function, we declare it as "void *function_thread(void *parameters)". The return value and the parameter
              type are kept as void pointers. It is for the convenience because the thread function can act as a generic function for any data
              type. For example, if we create a thread for adding two numbers in a process, we can typecast the integer or float or double
              pointer to a void type and pas it to the thread as a parameter. But, if we would have kept explicit types of the thread function,
              we will have to build three different functions. This is why the return types and the parameter type of the thread is "void*"
              type.
Scheduling:
  References:
    - https://www.tutorialspoint.com/operating_system/os_process_scheduling.htm
    - https://www.studytonight.com/operating-system/process-scheduling
    - https://www.guru99.com/cpu-scheduling-algorithms.html
    - https://www.studytonight.com/operating-system/cpu-scheduling-algorithms
    - Operating System Concepts, by Silberschatz, Galvin: Chapter 5
  Practice_questions:
    - Question: Of the three types of schedulers present in an OS, which executes most frequently?
      Options:
        - 1: Short Term Scheduler
        - 2: Mid Term Scheduler
        - 3: Long Term Scheduler
      Company_tags:
      Level:
      Reference:
      Correct_answer: 1
      Answer:
        - Hint:
        - Solution: |
            Short Term Scheduler is executed most frequently because it is responsible for selecting the next process to be executed. It is
            also known as dispatcher.
    - Question: Long Term Scheduler controls the degree of multiprogramming. True or False?
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: "True"
        - 2: "False"
      Correct_answer: "True"
      Reference:
      Answer:
        - Hint:
        - Solution: |
            It is true because the long term scheduler is responsible for selecting a process to be brought into the ready queue. Now, if
            the long term scheduler is not called frequently enough, the ready queue may become empty, and thus there may not be multi-
            programming.
    - Question:
        S1: System Library (libc.so) is linked at compile-time, and Shared Library (libm.so) is linked dynamically.
        S2: If a process is swapped, it can return to any address at swapping back to the main memory.
        Please select the correct option, where “TRUE/TRUE” indicates that S1 is TRUE, and S2 is TRUE.
      Company_tags:
      Level:
      Options:
        - 1: TRUE/TRUE
        - 2: TRUE/FALSE
        - 3: FALSE/TRUE
        - 4: FALSE/FALSE
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              The statement S1 is true and is a fact. The second statement is not always true. The swapped out process must return to the same
              address if they had binding that was done compile time or load time. This is the reason why we share a driver file and not
              the compiled file.
    - Question: What are the conditions under which a non-preemptive process leaves the CPU (i.e., a voluntary preemption occurs)?
      Company_tags:
      Level:
      Reference:
        - 1: https://www.geeksforgeeks.org/preemptive-and-non-preemptive-scheduling/
      Answer:
        - Hint:
        - Solution: |
            - When the process reaches termination.
            - When the process voluntarily goes into the waiting state.
            Note: Except for these conditions, rest all conditions lead to non-voluntary preemption.
    - Question: What is the difference between a scheduler and a dispatcher?
      Company_tags:
      Level:
      Reference:
        - 1: https://www.geeksforgeeks.org/difference-between-dispatcher-and-scheduler/
      Answer:
        - Hint:
        - Solution: |
            A dispatcher comes into action after a short-term scheduler selects a process that will get a chance to execute in the CPU. Thus,
            the dispatcher is a module that gives a process control over the CPU. It involves the following steps:
            - Switching context
            - Switching to user mode
            - Jumping to the proper location in the user program to restart that program.
    - Question: What are the Scheduling Criteria that we need to keep in mind while designing a scheduling algorithm?
      Company_tags:
      Level:
      Reference:
        - 1: https://www.youtube.com/watch?v=bWHFY8-rL5I
      Answer:
        - Hint:
        - Solution: |
            - CPU Utilization: Making sure that the maximum of the CPU is utilized most of the time and that utilization is productive. As we
                               will see in deadlocks, the CPU may be executing a wait function indefinitely, and then though it will be busy,
                               it will not be productive.
            - Throughput: We need to be sure that the throughput of the system is maximum.
            - Turnaround Time: If a process is submitted, then the time till it's completion should be minimum.
            - Deadlines: A maximum number of processes must meet their deadlines (real-time processes).
            - Response Time: If a process requests something, then it must be granted that as soon as possible.
    - Question: |
        Convoy Effect is when a process with a large CPU burst is executed ahead of processes with short CPU bursts; it causes starvation in
        for the processes with shorter burst time. Will there be a "Convoy Effect" involved in (First-Come-First-Serve)FCFS scheduling?
        Assume that all the processes involved in this scheduling are CPU bound only, and there is no quantum time for any process.
      Company_tags:
      Level:
      Options:
        - 1: "Yes"
        - 2: "No"
      Reference:
      Correct_answer: 1
      Answer:
        - Hint:
        - Solution: |
            Yes. FCFS involves a "Convoy Effect" when the processes are strictly CPU bound. This is because if the process with longer CPU
            burst arrives first then it will not exit the CPU until it has finished its execution.
            For e.g., Let there be three processes P1, P2 & P3 with CPU execution time as 500 ns, 50ns & 60ns, respectively, they arrive in
            order P1, P2, P3, respectively. This will mean that if P1 arrives first, it will execute for 500ns, and then only P2 will
            execute and then P3. This will cause starvation for P2 and P3. Now, if P2 was admitted to the ready queue first, then P3 and
            then P1, there would not have been that much starvation. This shows that FCFS shows "Convoy Effect".
    - Question: What is the main disadvantage of the Shortest Job First (SJF) Scheduling Algorithm?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            What SJF algorithm does is takes the process with the least next CPU burst as the next process to be executed. But, as we see
            the main problem here is knowing what will be the next CPU burst of any process. We cannot predict this with 100 percent
            accuracy.
    - Question: |
        The given table describes the arrival time of a process and the amount of time that the process takes to execute completely. Assume that all
        of these processes are CPU bound. Using the scheduling algorithm "Shortest Remaining Time First", answer following the questions.
                      -------------------------------------------
                      | Process | Arrival Time | Execution Time |
                      -------------------------------------------
                      |    A    |       0      |       3        |
                      |    B    |       2      |       6        |
                      |    C    |       4      |       4        |
                      |    D    |       6      |       5        |
                      |    E    |       8      |       2        |
                      -------------------------------------------
        For example, the first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: B
        - 2: C
        - 3: D
        - 4: E
      Correct_answer: 4
      Answer:
        - Hint:
        - Solution: |
            Only process 'A' is available in the ready queue initially, the CPU will start executing it. At time = 2, process 'B' will
            enter the ready queue, and that time process 'A' will have remaining time as 1 and process 'B' will have it as 6. So, process 'A'
            will continue executing. It will finish execution at time = 3 units. Only process 'B' exists in the ready queue at that time. So,
            it will start executing, and at time '4' units, process 'C' will enter the ready queue. Now, process 'C' has an execution time of
            4 units and process B has a remaining time of 5 units. This means that process C has the shortest remaining time. Proecss starts
            executing and at time = 6, process D enters with the time of execution as 5 and process C has a remaining time of 2 units and process
            B has a remaining time of 5 units. So, process C will continue to execute and finish at time = 8. At that time, we will have 3
            processes in the ready queue, B, D, and E. The process with the shortest remaining time is process E/ So it will finish its
            execution and then we have to processes with the same remaining time. It will then depend on the algorithm implementation to
            decide if process B executes or D. Let us assume that process B will execute as it was admitted before process D. This implies
            that the sequence of completion of execution is in the order: A, C, E, B, D.
    - Question: |
        Given table describes the arrival time of a process and the amount of time that the process takes to execute completely. Assume that all
        of these processes are CPU bound. Using the scheduling algorithm "Shortest Remaining Time First", answer following the questions.
                      ---------------------------------------------------------------------
                      | Process | Arrival Time | Execution Time | I/O Time |  Time to I/O |
                      ---------------------------------------------------------------------
                      |    A    |       0      |       7        |     5    |       3      |
                      |    B    |       2      |       9        |     2    |       5      |
                      |    C    |       4      |       6        |     0    |       6      |
                      |    D    |       6      |       8        |     1    |       4      |
                      ---------------------------------------------------------------------
        For example, the first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        I/O time zero means that it is a CPU bound process. Also, if process x has 'a' units of time to I/O, it means that it will execute
        for 'a' time units first and then go for I/O.
        What will the decision points here? Which process will finish its execution first? At what time will it do that? What is the main
        disadvantage of using this algorithm?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            The decision points here will be the arrival of a process and remaining burst time.
            As for the main disadvantage, after solving the problem and drawing a Gnatt chart, we can come to the conclusion that the number
            of decision points is too much. This means context switching will mostly be high in general scenarios. This will reduce the
            efficiency of the system.
            Now let's go through the scheduling. As process A will arrive first, it will be the first one to be executed. At time = 2 process
            B will arrive. Now, process A has 1 second left (of the time quantum) to execute and process B has 5. So, process A will continue
            execution until time = 3. Then it will go for I/O and will retuirn at time = 8. At that time, only process B will be available in
            the ready queue and will start executing. At time = 4, process C comes in ready queue. Now, process B has 4 units of time
            remaining before going to I/O, which is less than what process C will execute for. So, process B will continue to execute until
            time = 6 because that is when process D enters. Now, as B has the least remaining time, i.e., 2 time units, it will execute and
            go for I/O and return at time = 10. Now comes the crucial point where process A returns, and there are processes C and D already
            in the queue. Based on the remaining execution time, process A will execute till time = 10 as that is when process B return and
            this makes that instance a decision point. Still process A will continue to execute as it has the least remaining execution time
            and will go for I/O at time = 11 and return at time = 16. The process with least remaining execution time is B (9-5=4) and D.
            Let us assume that process D executes instead of B because this will depend on how tiebreaker is decided in the algorithm
            implementation. Now, it will continue to execute till time = 15 and go for I/O and return at time = 16. As no other process is
            admitted to the ready queue or returns from I/O. Then process B will execute for 1 time unit (it being the process with least
            remaining time) as process A returns from I/O at time = 16. Process A being the one with the least remaining time (7-6=1) will
            execute and complete. Here, we need to understand that had process D executed instead of process B; process A would have been the
            first one to finish its execution completely and exit. Instead, process B becomes the one to finish its execution first and exit.
            Now, process D will execute and go for I/O and come back after 1 time unit and then again execute for 3 time units and complete
            its execution and exit and then process C which got preempted by the arrival of process D D from I/O will execute for the
            remaining time as it is a CPU bound process and its total execution time is equal to its time quantum.
    - Question: |
          What is the effective access time of memory when there is an 80% hit in TLB and the access time for TLB is 20ns and that of the
          memory is 100ns?
        Company_tags:
        Level:
        Reference:
        Options:
          - 1: 140ns
          - 2: 150ns
          - 3: 80ns
          - 4: 160ns
        Correct_answer:
        Answer:
          - Hint:
          - Solution: |
              The correct answer is option b. Hit percentage means out of 100, how many memory accesses were present in TLB. For example,
              if the hit percentage is 80%, 60% of the time, we find what we need to access and the other 20% of the time, we do not and hence
              there is an exception, and the CPU takes over, loads that memory page location into the TLB, and again executes the instruction.
              Therefore, 80% hit means 80% successful accesses but, out of 80%, the first 60 are successful the first time and the remaining
              20 the second time, after the CPU loads them into the TLB. No, that we are clear with hit percentage, the question is pretty simple.
              Effective access time = ((hit%*(TLB access time + Memory access time))  + ((1-hit%)*(Memory access time + TLB access time)))/hit%
              = (0.8*(100+20) + (100+20))/0.8 = 150ns.
              Now we have multiplied (1-hit%) with 120 because, once it will search the TLB for the page location, then when it will not find it,
              it will access the memory and load it into TLB.
    - Question: What will the decision points for a non-preemptive scheduling algorithm?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            The decision points will be time quantum expiry, going for I/O, termination (in which which case it will exit).
    - Question: |
        The given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Using the non-preemptive version of Priority Scheduling, answer following the questions.
                      ------------------------------------------------------
                      | Process | Arrival Time | Execution Time | Priority |
                      ------------------------------------------------------
                      |    A    |       0      |       3        |     5    |
                      |    B    |       2      |       2        |     3    |
                      |    C    |       4      |       5        |     2    |
                      |    D    |       6      |       4        |     4    |
                      |    E    |       8      |       1        |     1    |
                      ------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
              The priority used are just symbolic; in real systems, they may be different.
        What will be the order of completion of the processes?
      Company_tags:
      Level:
      Options:
        - 1: A,B,C,D,E
        - 2: E,C,B,D,E
        - 3: A,B,C,E,D
        - 4: B,A,C,E,D
      Reference:
      Correct_answer: 3
      Answer:
        - Hint:
        - Solution: |
            For the solution, "time" is denoted by the symbol 't'.
            The solving technique will not be much different from a non-preemptive scheduling algorithm. This execution will start with the
            process A. At t=2, process B will arrive but because this scheduling is not preemptive, process A will keep executing till t=3.
            At this time, only process B remains in the ready queue, so it will start it execution and continue its execution till t=5. At
            t=5, only process C is available in the ready queue so it will start execution and continue till t=10. At this point of time,
            there will be process D and E in the ready queue. As the process E is of higher priority than E, it will execute before process
            D and hence process E will finish at t=11 and then process D finishes at t=15.
            So the order of completion is A,B,C,E,D.
    - Question: |
        The given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Instead, now use the preemptive version of Priority Scheduling to answer the following questions.
                      ------------------------------------------------------
                      | Process | Arrival Time | Execution Time | Priority |
                      ------------------------------------------------------
                      |    A    |       0      |       3        |     5    |
                      |    B    |       2      |       2        |     3    |
                      |    C    |       4      |       5        |     2    |
                      |    D    |       6      |       4        |     4    |
                      |    E    |       8      |       1        |     1    |
                      ------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        Will the decision points still be the same, in comparision to Shortest Remaining Time First (SRTF)? If not, what extra decision points do we
        have to take care of?
        Will the order of completion of processes change? Explain.
      Company_tags:
      Level:
      Options:
        - 1: Will change.
        - 2: Will not change.
      Reference:
      Correct_answer: 1
      Answer:
        - Hint: Priority Scheduling + Priority
        - Solution: |
            The point that we need to take care of is that the arrival of a higher priority process will cause preemption. We can think of
            this decision point as a replacement of the decision point " the arrival of a process with lesser remaining time" in SRTF.
    - Question: Can we, techinically speaking, implement FIFO Scheduling using Round Robin?
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: "Yes"
        - 2: "No"
      Correct_answer: 1
      Answer:
        - Hint:
        - Solution: |
            FIFO is just Round Robin with infinite time quantum. So, yes, we can implement FIFO using Round Robin.
    - Question: |
        Is there any relation between time quantum and the context switching time? Explain.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            Yes, there is. We cannot keep the quantum time very large because this will mean that processes that take a longer time to execute
            will occupy the CPU for a longer time and cause starvation. On the other hand, if we keep it too short, the throughput of the
            CPU will decrease as the majority of the time will go in context switching. Roughly speaking, we can say that time quantum must be
            kept such that it is sufficiently large than the context switching time but not too large. Time quantum is generally 1000 times
            the context switching time.
    - Question: |
        The given table describes the arrival time of a process and the amount of time that process takes to execute completely. Assume that all
        of these processes are CPU bound. Use Preemptive Priority Scheduling with Round Robin to answer the following questions.
                      ------------------------------------------------------
                      | Process | Arrival Time | Execution Time | Priority |
                      ------------------------------------------------------
                      |    A    |       0      |       4        |     4    |
                      |    B    |       2      |       5        |     3    |
                      |    C    |       4      |       8        |     2    |
                      |    D    |       6      |       7        |     3    |
                      |    E    |       8      |       3        |     4    |
                      ------------------------------------------------------
        Note: The first row says that process A arrives at t=0 in the ready queue and will take 3 time units to execute completely.
        The time quantum for Round Robin is 2 time units.
        Which process is the last to finish execution?
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: B
        - 2: C
        - 3: D
        - 4: E
      Correct_answer:
      Answer:
        - Hint:
        - Solution: |
            The execution starts with process A and at t=2, because B process enters the ready queue the scheduler will check if it has to
            preempt process A. Since priority of B is more than that of process A (lower the priority number, more is the priority) this is
            why process A will be preempted and process B will begin execution. At t=4, process C is admitted to the ready queue and since
            its priority is more than that of process B, it will executed after preempting B. Then at t=6, process D enters but it will not
            be repalcing process C in CPU because its priority is less than thet of the currently executing process. This means that process
            C will keep on executing till t=8. Again as the priority of processes E is less than that of the currently executing process,
            process C, will continue execution till it finishes because no other process will be admitted after that. So, at time t=12 it will
            finish and process B will resume its execution (assuming that the process admitted first executes, in case more than one processes
            have the same priority). At t=15, process B will finish and process D will finish its execution as it is the process with the
            highest priority that remains. Then process A will execute and then process E. So, the order of completion is C, B, D, A, E
            respectively.
    - Question: |
        Assume that Round Robin Scheduling is used for scheduling processes. What will happen if a process forks after completion of half of
        its time quantum? What will be the time quantum for the child and the parent process? Will their remaining time quantum be related?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            When such a situation occurs, the remaining time quantum is distributed equally among the parent and the child processes. This
            means that if the parent process had time quantum of 100ms and it forked at t = 50ms, then both the parent and child will have
            the remaining time quantum as (50ms/2),i.e., 25ms. This is for this execution cycle only. This means that after the finishing of
            this time quantum, both the processes will have the time quantum according to their priorities.
    - Question: What are the types of scheduling a process can be assigned to in a Linux kernel? Explain
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            A process can have one of the three types of scheduling in Linux, namely: Round Robin, FIFO and Other (or Normal). By default,
            the user-created processes have their scheduling policy as other. We can change this by the function sched_setparam().
            The Real-Time process are the ones that are scheduled either according to Round Robin or FIFO.
    - Question: Can we change the static priority of a process? If yes, how? If no, why?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            Yes. We can change the static priority of a process by changing the nice value of that process. This is because the static
            priority is calculated directly by the nice value of the process. Static Priority = 120 + Nice Value. So, if we change the nice
            value using the nice() system call, we change the static priority of the process.
    - Question: What will be the dynamic priority of a process with a base time quantum of 420ms, and the Bonus value is 5?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            Dynamic Priority is calculated as static priority - Bonus + 5. We are given the time quantum as 420ms. The base time quantum is
            calculated as (140 - static priority)*20 if the static priority is less than 120. Using this, we get the static priority as 119.
            The base quantum time is calculated as (140 - static priority)*5, but here the static priority is more than equal to 120. But, if
            use this formula, we get the static priority as negative, which is not possible. Hence, we went with the former formula. Using
            this, the dynamic priority is calculated as 119.

    - Question: How is the property "Bonus" of process related to its scheduling?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            The dynamic priority is calculated as max(100,min(static priority - Bonus + 5,139)) and dynamic priority is what is used while
            scheduling a process. This is why it is an important factor in scheduling.
            Note: 
              - 1: Let us break the dynamic priority function and understand it. The "max" function ensures that the min dynamic priority is
            100. Also, the "min" function ensures that the max value of dynamic priority is 139. This actually is done so that the range is
            confined to the priority numbers allowed for fair share scheduling. Only the processes with fair share scheduling have to care
            about this. Real-time processes are time-bound and their priority numbers range from 0-99. This is why we need to have such a
            formula for dynamic priority.
              - 2: The attribute "Bonus" of a process is related to the average sleeping time of a process. The more the average sleeping
            time of a process, the larger is its "Bonus" value.
    - Question: |
        Is there a relation between base time quantum of a process and its priority? If yes, what will the base time quantum of a process with
        static priority 121?
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: No Relation
        - 2: There is a relation. The base time quantum will be 380ms.
        - 3: There is a relation. The base time quantum will be 95ms.
        - 4: There is a relation. The base time quantum will be 200ms.
      Correct_answer: 3
      Answer:
        - Hint:
        - Solution: |
            The base time quantum of a process has two formulae depending on its static priority. For a process with a priority less than 120, it
            is calculated as (140-static priority)*20. And for static priority more than equal to 120, it is calculated as
            (140-static priority)*5.
            Now, if the static priority is 121, the base time quantum of a process will be (140-121)*5 = 95ms.
            Note: The calculated time quantum is always in millisecands(ms).
    - Question: |
        Will the average sleep time of a process which spends more time in CPU executing increase or decrease? Is there a relationship between the
        term "Bonus" and average sleep time?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            The process that will spend more the executing in CPU will see its average sleep time decreased. This is logical as more time it
            spends in CPU, lesser will be the fraction of time it will spend in I/O. And hence, lesser average sleep time.
            The "Bonus" is directly related to the average sleep time. The more the sleep time, the more the "Bonus" value.
            Note: Since the "Bonus" value will be more will be the dynamic priority will be less, and hence it will get more priority to
            execute. In this way, the CPU balances between a CPU bound process and an interactive process.
    - Question: Is a process with static priority 130 and a bonus value of 5, an interactive process? Explain.
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            There is a term called Interactive Delta. It gives an indication if a process if an interactive process or not.
            [(Static Priority/4)-28] is called the interactive delta of a process. In principle, if a process satisfies the following
            condition, it qualifies as an interactive process. The condition is: Dynamic Priority < [3*(Static Priority/4)_28]. This
            expression is simplified and we get (Bonus -5) > Interactive Delta, this is because Dynamic Priority is
            (static-priority - Bonus + 5). Now, if we calculate the interactive delta and it comes out to be less than (bonus - 5), we say
            that the process is interactive.
            For a process with static priority 130, Interactive Delta comes out to be 4.5, which is more than (5-5,i.e, Bonus - 5). So, a
            process with a given static priority and bonus value is not an interactive process.
    - Question: |
        Context: Expired Array and Active Array are used by Linux Scheduler to schedule Real Time processes.
        Are the data structures used for maintaining an expired process array and an active process array same or different? If yes, what is
        the advantage of keeping them same?
      Company_tags:
      Level:
      Reference:
      Answer:
        - Hint:
        - Solution: |
            The data structures used for both the queues is the same. This is because it keeps the scheduling a O(1) time complexity job.
            The reason is that while bringing the processes from the expired proecss queue to the active queue will become a O(n) time
            and to avoid this, we keep the data structures the same and just flip the pointers of the two queues. The accessing is always done
            relative to the head of the array, so when you simply change the pointers, you effectively change the queues.
Synchronization:
  References:
    - Operating System Concepts, by Silberschatz, Galvin: Chapter 5
  Practice_questions:
    - Question: |
        Consider that there are two processes that want to communicate with each other. Let the processes be called P1 and P2 respectively. If
        P1 sends a message to P2 and moves on without waiting for acknowledgement from P2, which of the following type of communication is it?
      Options:
        - 1: Synchronous Communication
        - 2: Asynchronous Communication
      Company_tags:
      Level:
      Reference:
      Correct_answer: 2
      Answer:
        - Hint:
        - Solution: |
            There are two types of communication that can happen in between processes, Synchronous (Blocking) and Asynchronous (Non-Blocking).
            The first type means that the sender waits for acknowledgement from the receiving process. This will mean that Asynchronous
            communication will mean that the sender will not wait for the acknowledgement from the receiving process. So, the type of
            communication between P1 and P2 will be Asynchronous.
    - Question: |
        Two processes P1 and P2 want to either read or write the same file. Which of the following options will not lead to a data
        inconsistency?
      Options:
        - 1: P1 will write and P2 will write
        - 2: P1 will write and P2 will read
        - 3: P1 will read and P2 will write
        - 4: P1 will read and P2 will read
      Company_tags:
      Level:
      Reference:
      Correct_answer: 4
      Answer:
        - Hint:
        - Solution: |
            Even if one process writes, there will be data inconsistency. All processes can read the same file at the same time because they
            will not be modifying the content. This is why they will read the correct values. Whereas, when either one process writes, due to
            writing section being a critical section, the other process reading/writing the same section will surely lead to an inconsistency.
    - Question: Let there be two processes P1 and P2 running concurrently.
          P1--->                       P2--->
          int main(){                  int main(){
            for(int i=0;i<50;i++)         for(int i=0;i<50;i++)
              x++;}                           x--;}
          The variable "x" is a shared variable and its initial value is "0". What will be the minimum value of "x" after both the processes
          have finished their execution?
      Options:
        - 1: "0"
        - 2: "-50"
        - 3: "-51"
        - 4: Can't say
      Company_tags:
      Level:
      Reference:
      Correct_answer: 2
      Answer:
        - Hint: Context Switching possible to nullify the effect of x++
        - Solution: |
            “x--” essentially means,
            load 'x' in a reg; ---(1)
            Reduce the 'reg' by '1'; ---(2)
            store 'x' in the 'reg';  ---(3)
            Now if there is a context switch after (1), then if “x++” executes and then (2) and (3) executes, the value in “reg” will be zero
            before context switch, and after executing “x++” also, “reg” will contain zero. And hence after stages 2 and 3, the final value
            of x will be “-1”.
            Keping this in mind, we can say that the minimum value of "x" will occur when this type of context switch occurs and each of these
            times, this value is one less. So, after 50 interations, the minimum value will be -50.
    - Question: |
        Based on the Producer-Consumer Problem, answer the following question.
        Assume that the algorithm for the Producer is:
        while (1) {
          // Produce item;
          Buffer[in] = item;
          in = in + 1;
        }
        Ans the algorithm for Consumer is:
        while (1) {
          while(in==out);
          item = Buffer[out];
          out = out + 1;
        }
        Note : The "Produce item" is the critical section.
        What is the problem with this simple algorithm?
      Options:
      Company_tags:
      Level:
      Reference:
      Correct_answer:
      Answer:
        - Hint: Context Switch.
        - Solution: |
            We need to keep in mind the Consumer and the Producer are two different processes. So, if the Producer just keeps on producing,
            and the Consumer does not get time to consume ,which will happen only when there will be a context switch (in case of a
            uni-processor system). Whereas in a multi-processor system, the two processes can run simultaneously and there will be no
            monitoring if consumer has confumed more than what the Producer is producing. This is the fundamental problem of this simple
            algorithm.
    - Question: Which of the following will not result in data inconsistency?
      Options:
        - 1: A file being opened by two processes simultaneously and one process is reading it and the other is writing in it.
        - 2: A file being opened by two processes simultaneously for reading.
        - 3: A file being opened by two processes simultaneously and both the processes modifying it.
      Company_tags:
      Level:
      Reference:
      Correct_answer: 2
      Answer:
        - Hint:
        - Solution: |
            Reading from a file not a critical section job. More than one process can simultaneously read from a file but if one file is
            reading and the others are writing, this can lead to wrong data being read.
    - Question: |
        Assume there are two processes executing with the following code:
        Process 1:
          int main()
          do{
            while (turn !=0); // do nothing
            //critical section
            turn = 1;
            //reminder section
          }while(1);
          Process 2:
          int main()
          do{
            while (turn !=1); // do nothing
            //critical section
            turn = 0;
            //reminder section
          }while(1);
          Is this process ideal for dealing with two processes trying the same critical section? Explain your answer.
      Options:
        - 1: "Yes"
        - 2: "No"
      Company_tags:
      Level:
      Reference:
      Correct_answer: 2
      Answer:
        - Hint: What happens if process 1 dies abruptly?
        - Solution: |
            The first problem with this algorithm is that these two processes must strictly alnernate between each other to continue execution.
            The other problem is that if one process ends abruptly, the other will wait eternally for the variable turn to change and hence
            will be stuck. There need to be a mechanism to make sure that if one process dies, the execution does not stop.
    - Question: |
        Consider two processes P1 and P2. The initial value of flag[0] and flag[1] is "False" and it is a shared boolean variable.
        Process P1:
          while(1){
            flag[0]=true;
            while (flag[1]);
            //critical section
            flag[0]=false;
            //reminder section
          }
        Process P2:
          while(1){
            flag[1]=true;
            while (flag[0]);
            //critical section
            flag[1]=false;
            //reminder section
          }
          Is there a flaw in this algorithm? Justify your answer.
      Options:
        - 1: "Yes"
        - 2: "No"
      Company_tags:
      Level:
      Reference:
      Correct_answer: 1
      Answer:
        - Hint: Possibility of an infinite loop?
        - Solution: |
            If at any point in the running of this algorithm, both flag[0] and flag[0] become "True", the algorithm will go in an infinite
            loop. This algorithm satifies "Mutual Exclusion" property of an algorithm that avoids Race Condition. But, this may lead to a
            Dead Lock.
    - Question: Which of the following will guarantee a deadlock in a single instance resource?
      Company_tags:
      Level:
      Reference:
      Options:
        - 1: Mutual Exclusion
        - 2: Circular Wait for graph
        - 3: Hold and Wait
        - 4: Preemption of one of the process.
      Correct_answer: 2
      Answer:
        - Hint: Single instance resource and circular wait are related.
        - Solution: |
            The correct option is b. This is because if every process in the graph is holding a resource and requesting for other and another
            process is requesting for the resource, the first is holding there is a definite deadlock. The others do not guarantee that.
DSTN:
  References:
    - Textbook_1: Storage Networks The complete Reference. Robert Spalding, McGraw-Hill/Osborne, 2003.
    - Textbook_2: Information Storage and Managements Storing, Managing and Protecting Digital Information. EMC Education Services, Wiley
                  Publishing Inc., 2008.
    - Textbook_3: Storage Networking-Real World Skills for the CompTIA Storage + Certification and Beyond by Nigel Poulton, Publishers,
                  SYBEX a Wiley brand, 2014
    - Videos and Blogs:
  Memory Management:
    Practice_questions:
      - Question: |
            S1: If an entry is not found in the page table, the page table is not invalid. Rather, it is fetched because loading a Page
                Table is a dynamic job.
            S2: The frame size in a frame table can be 4KB or 4MB in a 32-bit system.
        Options:
          - 1: TRUE/TRUE
          - 2: TRUE/FALSE
          - 3: FALSE/TRUE
          - 4: FALSE/FALSE
        Reference:
          - 1: https://cs.stackexchange.com/questions/47541/is-page-size-always-equal-to-frame-size
        Company_tags:
        Level:
        Correct_answer: 1
        Answer:
          - Hint:
          - Solution: |
              If the frame entry is not found in a page table, we so to swap space and bring back the entry. But any entry that is beyond the
              Page Table Limit Register is invalid. This type of loading is done because it is dynamic in nature. The frame size depends on
              the implementation of the architecture. It is generally 4KB or 4MB. But, you can modify it to what you want.
      - Question: The user program deals with which type of address?
        Options:
          - 1: Physical Address
          - 2: Virtual Address
        Company_tags:
        Level:
        Reference:
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              The user program interacts with the CPU only with the virtual address. It is the Memory Management Unit that takes care of the
              address translation into Physical Address.
      - Question: If a program is written such that it only requires Compile Time Binding, which of the following statements is true?
        Options:
          - 1: The Virtual Address generated is different from the Physical address at that will be accessed at the time of execution.
          - 2: Virtual Address generated is same as the Physical address at that will be accessed at the time of execution.
          - 3: There is no relation between the two types of addresses in any type of binding.
          - 4: None of these
        Company_tags:
        Level:
        Reference:
        Correct_answer: 2
        Answer:
          - Hint: Programs compiled with compile time binding can only run at in the systems that they are compiled in.
          - Solution: |
              At Compile time the address generated is equal to the virtual address in the case of compile time binding.
      - Question: Which of the following scheduler is responsible for swapping?
        Options:
          - 1: Short Term Scheduler
          - 2: Long Term Scheduler
          - 3: Mid Term Scheduler
          - 4: None of these because it is done by some independent hardware.
        Company_tags:
        Level:
        Reference:
        Correct_answer: 3
        Answer:
          - Hint:
          - Solution: |
              The Mid Term Scheduler is responsible for swapping out processes into the swapping space when the system is running short on
              memory. The Mid Term Scheduler then comes into action and swaps out the process from the memory. But remember, the PCB of the
              process is not saved. Only the code section, data section, etc. is swapped out.
      - Question: |
          If a process had compile time binding, and if the process is swapped, does the mid term scheduler save the PCB of the PCB?
        Options:
          - 1: Yes, it does.
          - 2: No, it doesn't.
        Company_tags:
        Level:
        Reference:
        Correct_answer: 1
        Answer:
          - Hint: Programs compiled with compile time binding can only run at in the systems that they are compiled in.
          - Solution: |
              Because the data section, code section have fixed addresses in compile time and load time binding, the complete context of the
              process needs to be saved if the process had compile time or load time binding.
      - Question: For dynamic partitioning of memory which of the following memory allocation method is the worst?
        Options:
          - 1: Best Fit
          - 2: Next Fit
          - 3: Worst Fit
          - 4: First Fit
        Company_tags:
        Level:
        Reference:
        Correct_answer: 1
        Answer:
          - Hint: Fragmentation
          - Solution: |
              The Best Fit algorithm finds a contiguous block of memory that is equal to slightly greater than the memory that we need to get
              allocated. This leaves small fragments of memory as holes and these fragments cannot be used. The best performing algorithm
              is the Next Fit algorithm.
      - Question: What does the term "inclusiveness" mean with respect to Cache Memory?
        Options:
        Company_tags:
        Level:
        Reference:
        Answer:
          - Hint: How are different levels of cache related?
          - Solution: |
              Inclusiveness with respect to Cache Memory means that L1 cache will be a subset of L2 cache, L2 cache will be a subset of L3
              cache, and so on. Each level of cache does not any data that is not available in the higher level of cache.
      - Question: |
          In what case will accessing L1 cache and getting a cache miss will result in accessing the Page Table directly?
        Options:
        Company_tags:
        Level:
        Reference:
        Answer:
          - Hint: Is there a TLB? Is there a Cache Miss?
          - Solution: |
              The TLB stores the page number and corresponding frame number for a page table entry. And, an entry in cache consists of data
              and tag bits. So, cache miss will imply that the tag bits don't match and hence it will mean that the CPU will reference the
              TLB. Therefore a miising TLB will mean that cache miss will result in a direct
              Page Table access.
      - Question: |
          In what case of pure paging, what is the significance of "modified bit"?
        Options:
        Company_tags:
        Level:
        Reference:
        Correct_answer:
        Answer:
          - Hint: Dirty page
          - Solution: |
              When the Modified bit or dirty bit is set, it means that the page data has been modified. It's data is not the same as it was
              when it was brought in memory. So, it must be written back and the page table should be updated, when the dirty page is accessed.
      - Question: |
          State ture or false.
          Statement: The "modified bit" is not required if the cache iplements write through mechanism of Memory Management.
          Explain your choice.
        Options:
          - 1: "True"
          - 2: "False"
        Company_tags:
        Level:
        Reference: https://stackoverflow.com/questions/27087912/write-back-vs-write-through-caching
        Correct_answer: 1
        Answer:
          - Hint: Why use a dirty bit when nothing is dirty?
          - Solution: |
              The answer lies in the mechanism of Write Through mechanism. This method ensures that the main memory has the up-to-date copy of
              the desired data in the main memory. This means that cache and memory are coherent and have consistent data. So, there is no
              need to keep a dirty bit as a part of the entry in Page Table or in Cache.
      - Question: What are the main advantages of paging?
        Options:
        Company_tags:
        Level:
        Reference:
          - 1: http://www.cs.iit.edu/~cs561/cs351/VM/paging%20advantages%20and%20disadvantages.html
        Correct_answer: 1
        Answer:
          - Hint:
          - Solution: |
              Some of the advantages of Paging are:
                - Separation of user view of memory & actual physical memory
                - Allows demand paging and prepaging
                - More efficient swapping
                - No need for considerations about fragmentation
                - Just swap out page least likely to be use
      - Question: Assume that a system implements Pure Paging with TLB as the Memory model. How is a TLB miss handeled in this system?
        Options:
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/37825859/cache-miss-a-tlb-miss-and-page-fault
        Correct_answer:
        Answer:
          - Hint: Page miss, invalid page, etc.
          - Solution: |
              TLB can be called a address-translation cache. So, a TLB hit implies that the requested page is mapped to a physical memory
              frame and that entry is a part of TLB and that entry is found, whose address is a part of the TLB entry. Similary, TLB miss
              the page-frame translation is not there in the TLB and hence, the MMU goes and fetches it from the page table. If the page is
              available in the page table, then the entry in TLB is updated and the instruction that caused the TLB miss is exected again.
              If the page is not available in the page table, then it is a page fault. Then the control is transferred to the OS. Page fault
              means that either the page is invalid or it is not present in the page table. If the page is invalid, the OS decides if the page
              is just invalid hence swap the invalid page for the valid page or if the requested page address is beyond the virtual address
              space of the process. If it the later case, the process is terminated. In the former case, the page table and the TLB is updated
              and the execution of the exception causing instruction begins again.
      - Question: Which of teh following is true?
        Options:
          - 1: Paging causes external fragmentation.
          - 2: All invalid page references lead to process termination.
          - 3: Paging cause internal fragmentation.
          - 4: Demand paging causes performance degradation
        Company_tags:
        Level:
        Reference:
        Correct_answer: 3
        Answer:
          - Hint:
          - Solution: |
              Paging causes internal fragmentation because the data is stored in frames of equal size and it is possible the the frames are
              not filled completely. Not all invalid page references lead to process termination. Demand paging is implemented to increase the
              efficiency of paging mechanism.
      - Question: If a system has exclusive cache, do the L1 cache and L2 cache have different cache block sizes?
        Options:
          - 1: Yes, because in exclusive cache, the L1 cache is a subset of L2 cache.
          - 2: No, because if there is L1 cache data is not a subset of L2 cache, data from L2 is brought into L1.
          - 3: No, because L1 cache is not a subset of L2 cache.
          - 4: Yes, because cache block sizes are never equal.
        Company_tags:
        Level:
        Reference:
          - 1: https://en.wikipedia.org/wiki/Cache_inclusion_policy#Exclusive_Policy
          - 2: https://cs.stackexchange.com/questions/47009/what-if-block-sizes-are-not-equal-among-caches
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              In exclusive caches the block sizes among different levels is same and this is being followed in real life commercial PCs.
              Intel architecture systems use Inclusive caches.
      - Question: |
          In TLB with ASIDs (Address Space Identifiers), do the entries related to a process get invalidated during a context switch? Explain.
        Options:
          - 1: "Yes"
          - 2: "No"
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/52713940/purpose-of-address-spaced-identifiersasids
          - 2: https://community.arm.com/developer/ip-products/processors/f/cortex-a-forum/5229/address-space-identifier---asid
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              No. In conventional TLB, the entries related to a process get invalidated when there is a context switch. But with ASIDs, the
              entries get invalidated when the process is terminated.
      - Question: Which of the following is true regarding Look Aside Cache?
        Options:
          - 1: If the processor does not find the required data in L1 cache, it will fetch it from L2 cache.
          - 2: It is slower than Look Through cache.
          - 3: Processor cannot look for data in more than one levels of cache at the same time.
          - 4: None of these
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/34001002/look-through-vs-look-aside
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              Look Aside cache is a cache in which the processor looks for the data in all levels of cache and main memory simultaneously.
              When there is a memory hit, it has to send a request cancellation signal to the other levels and main memory (if data is data is
              found in cache). Now-a-days cache has a hit rate of approximately 95%, this means that 95% of the time, the requests need to be
              cancelled and this contributes to a significant overload. Hence, it is slower than look through cache and many of today's
              systems use it.
      - Question: What is the difference between Linear Address and Logical Address?
        Options:
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/15851225/difference-between-physical-logical-virtual-memory-address
        Correct_answer:
        Answer:
          - Hint:
          - Solution: |
              Today, all the machines work on stored program concept. This means that all the programs that are running must reside in the
              Main Memory, which is also called the Physical Memory. Now, since all of today's systems are multicore-multiprocessor sytems,
              many processes run simultaneously in the processor (as whole). it is obvious that 8GB-32GB cannot hold all the processes
              completely. This is why when a process is started, it has it's own virtual address space. This Virtual Address is mapped to
              Physical Memory and since there is dynamic loading, not all of the process needs to reside in the Main Memory. So, the memory
              that user is able to see is the virtual address, which has a mapping to the main memory because a program cannot run unless it
              resides in the main memory (PCB does this trick).
              Linear Address is an intermediate address of sorts that is generated from the Virtual(Logical) Address. When we use segmentation
              the Virtual Address consists of the segment number and offset. This segment number tells the entry in the segment table. This
              entry gives the base address of the segment and this is then added to the offset in the Virtual Address.
              This is the difference between the two.
      - Question: Which of the following is not true regarding segmentation?
        Options:
          - 1: Since segments are not equal, segmentation is similar to dynamic partitioning.
          - 2: Segmentation is very difficult to implement in systems with growing data structures.
          - 3: Programs can be altered and recompiled easily.
          - 4: Data sharing is easy and protection can also be implemented easily.
        Company_tags:
        Level:
        Reference:
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              The main advantage of segmentation is the dynamic size of segments. They need not be of equal size. This means that there is no
              external fragmentation in segmentation. This is why it is ideal for growing/changing data structures (i.e, the ones which
              change in size).
      - Question: In Segmentation with Paging, the Segment Table entry contains segment number. Is this statement correct?
        Options:
          - 1: "NO"
          - 2: "YES"
        Company_tags:
        Level:
        Reference:
          - 1: https://www.javatpoint.com/os-segmented-paging
        Correct_answer: 1
        Answer:
          - Hint:
          - Solution: |
                When Segmentation is combined with Paging, we get a powerful Memory Management technique. In this technique, the memory is
                divided in various segments which are in turn divided in many pages. Note that the segments need not be of fixed sizes but the
                pages are of fixed size. Since a segment is made up of many pages, every segment has a page table. This table has the further
                translation of the address requierd.
                To explain Segmentation+Paging in short, the CPU generates the logical address which is converted into Linear Address by the
                Segmentation Unit and this is an input to the Paging Unit and it gives the physical address corresponding to the Logical
                address generated by the process (CPU).
      - Question: |
          Assume that the page fault rate is 0.001, Memory access time is 200 ns (including TLB access time), average page-fault handling time
          is 8 ms (including TLB miss and the resulting overhead). What is the Effective Access Time (EAT) of the memory?
        Options:
          - 1: 80 microseconds
          - 2: 8.2 microseconds
          - 3: 6 microseconds
          - 4: 10 microseconds
        Company_tags:
        Level:
        Reference:
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              Page Fault rate 0.001 means that 1 out of every 1000 page access referencing is a page fault. So, EAT = (0.999*200ns)+(0.001*8ms).
              This comes out 8.2 microseconds approximately. Remember that the "8 microseconds" include page fault handling overhead, page swap
              in and out and the instruction restart overhead.
              Notice the this is a degradation of over 40% in performance. To have a 10% increase in performance, we will need to have a
              page fault rate of 0.0000025, meaning that we need to implement the algorithm such that it results in minimum page faults.
              0.0000025 rate of page faults means 1 page fault in 400,000 accesses.
      - Question: |
          State true or false.
          Statement: If a system uses LRU as the replacement policy for page faults, every time a page is brought in memory that time is used
                     to make the decision of whether to keep the page while replacement or not.
        Options:
          - 1: "True" 
          - 2: "False"
        Company_tags:
        Level:
        Reference:
          - 1: https://www.geeksforgeeks.org/page-fault-handling-in-operating-system/
          - 2: https://www.geeksforgeeks.org/program-for-least-recently-used-lru-page-replacement-algorithm/
        Correct_answer: 2
        Answer:
          - Hint:
          - Solution: |
              The statement is false. This is because the LRU algorithm uses the time when the page is referenced (or the memory is referenced)
              as the decision point. FIFO uses the time of bringing in the page as the decision point.
      - Question: Which of the following gives better throughput, Global replacement of frames or Local replacement of pages? Justify.
        Options:
        Company_tags:
        Level:
        Reference:
          - 1: https://www.geeksforgeeks.org/operating-system-allocation-frames/
        Correct_answer:
        Answer:
          - Hint:
          - Solution: |
              Global replacement will give better throughput because it can take frames allocated to other processes as well. This means that
              that process will complete its task as soon as possible. This has a downside that the number of page faults may increase. Despite
              this, the Global replacement policy tends to perform better (i.e., give a better throughput).
      - Question: |
          Suppose there are two replacement policies for Cache: LRU and FIFO. There will be a data structure for both of these policies.
          If there is a hit, which of the following data structures will(may) change, as in which of the data structures will be updated?
        Options:
          - LRU
          - FIFO
        Company_tags:
        Level:
        Reference:
        Correct_answer: 1
        Answer:
          - Hint:
          - Solution: |
              The LRU replacement algorithm works on the principle of "least recently accessed" entry. If there is cache hit, it will mean
              some entry in the cache was accessed. This means that the LRU counter will change and the LRU entry may or may not change. LRU
              entry will change if the most recently accessed cache entry was the LRU before the access. But the LRU counter will definitely
              change. As for the data structure used for FIFO, it will not change as the replacement only depends on when the entry was made
              and not on when it was accessed.
  File System:
    - File systems:
        Practice_questions:
        - Question: A directory in LINUX is techinically a file. Is this statement correct? Justify.
          Options:
            - "True"
            - "False"
          Company_tags:
          Level:
          Reference:
          Correct_answer: 1
          Answer:
            - Hint:
            - Solution: |
                This statement is true. Everything in LINUX can be classified as either a process or a file. A directory is a file whose
                content is links to the files in it. These files can be "files" (as in the ones that are not directories) or directories
                (which, as we have come to know, are files). The structure of the file system in LINUX can be thought of as a graph. There
                are links among various nodes. The top is the root directory and the leaves are the files. They don't have anthing in them
                other than data.
        - Question: Can a system have the number of inodes as dynamic?. Give reasons.
          Options:
            - "Yes"
            - "No"
          Company_tags:
          Level:
          Reference:
          Correct_answer: 1
          Answer:
            - Hint: inode list.
            - Solution: |
                Every filesystem has a system wide inode list that stores the information about all the inodes that exist in the filesystem.
                Like, how many are free, how many are allocated. Now, this "inode list" has got a specific size. We we configure our
                filesystem, we specify the size and that size cannot be changed later. So, there will exist a predefined number of inodes in
                the filesystem at any point in time and hence their number is not variable.
        - Question: How many data blocks are read when the command "fopen("/DSTN/PrePlaced/student.txt","r")" is executed?
          Options:
            - 1: 3
            - 2: 4
            - 3: 8
            - 4: Can't say
          Level:
          Reference:
          Correct_answer: 2
          Answer:
            - Hint: Reading a data block different from reading inode.
            - Solution: |
                Let us understand the process in a bit of detail. First, the system will "fetch the inode of the root" since the address is
                absolute type. Then it will read the data block of root and search for the inode for the directory "DSTN". Then it will read
                the data block of DSTN and then search for the inode for "PrePlaced", subsequently reading it data block and searching for
                the inode of "student.txt". It will then read the first block of data as this is in "read mode". Had it been in append mode,
                it would have to have read the data block equal to the size of the file. Therefore, the total number of data block reads are
                "4".
        - Question: How many inodes are accumulated in memory when we execute the command "fopen("/DSTN/PrePlaced/student.txt","r")"?
          Options:
            - 1: 0
            - 2: 1
            - 3: 3
            - 4: As many inodes are read
          Company_tags:
          Level:
          Reference:
          Correct_answer: 1
          Answer:
            - Hint: Releasing of inode also takes place.
            - Solution: |
                When we get the next inodes so that we can read their corresponding data blocks, we release other inodes that we have accessed
                before so that other processes can use them and hence do not have to wait for one process to finish its adress translation.
                Therefore, only one inode is there in memory at a time.
                Note that this does not mean that it will be completely discarded, it will still reside in the cache until the entry in which
                it resides is updated (that is, the data of the entry is replaced).
Miscellaneous:
  - Practice_questions:
      - Question:
        Company_tags:
        Level:
        Reference:
          - 1: https://stackoverflow.com/questions/13185300/where-is-the-mode-bit
          - 2: https://stackoverflow.com/questions/28815848/does-each-core-has-its-own-private-set-of-registers
          - 3: http://www.cs.cornell.edu/courses/cs3410/2012sp/lecture/22-traps-i-g.pdf
        Answer:
          - Hint:
          - Solution: #Will write soon